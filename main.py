import os
import yaml
import datetime
import argparse
import time
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor
from src import hf_scraper, utils, llm_agent, zotero_ops, paper_analyzer
from rich.console import Console
from rich.live import Live
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn
from rich.panel import Panel
from rich import box

console = Console()

# å…¨å±€å˜é‡
ZOTERO_STRUCTURE = {}

async def process_paper_async(arxiv_id, meta, local_dir, target_date, skip_deep, progress, task_id, semaphores):
    """å¼‚æ­¥å¤„ç†å•ç¯‡è®ºæ–‡çš„æµæ°´çº¿"""
    filter_sem, download_sem, ocr_sem, llm_sem = semaphores
    
    try:
        # 1. RAG åˆ†æ (ç­›é€‰)
        progress.update(task_id, description=f"[cyan]ğŸ” ç­›é€‰ä¸­: {meta['title'][:30]}...")
        async with filter_sem:
            loop = asyncio.get_event_loop()
            analysis = await loop.run_in_executor(
                None, 
                llm_agent.analyze_paper_with_structure,
                meta['title'], meta['summary'], ZOTERO_STRUCTURE
            )
        
        if not analysis.get('interested'):
            progress.update(task_id, description=f"[grey50]â­ï¸  å·²è·³è¿‡: {meta['title'][:30]}", completed=100)
            return {
                "status": "ignored",
                "title": meta['title'],
                "reason": analysis.get('reason', 'ä¸æ„Ÿå…´è¶£'),
                "url": meta['pdf_url']
            }

        # 2. å‡†å¤‡ç›®å½•
        category = analysis.get('category', 'Uncategorized')
        category_dir = os.path.join(local_dir, category)
        os.makedirs(category_dir, exist_ok=True)
        
        short_title = utils.sanitize_filename(meta['title'])[:40]
        first_author = utils.sanitize_filename(meta['authors'][0])
        paper_subdir_name = f"{first_author}_{short_title}"
        paper_dir = os.path.join(category_dir, paper_subdir_name)
        os.makedirs(paper_dir, exist_ok=True)
        
        filename = f"{first_author}_{short_title}.pdf"
        pdf_path = os.path.join(paper_dir, filename)

        # 3. ä¸‹è½½ PDF
        progress.update(task_id, description=f"[blue]ğŸ“¥ ä¸‹è½½ä¸­: {meta['title'][:30]}...", advance=20)
        async with download_sem:
            download_success = await loop.run_in_executor(None, utils.download_pdf, arxiv_id, pdf_path)

        if not download_success:
            print(f"   âŒ PDFä¸‹è½½å¤±è´¥: {meta['title'][:40]}...")
            progress.update(task_id, description=f"[red]âŒ ä¸‹è½½å¤±è´¥: {meta['title'][:30]}", completed=100)
            return {
                "status": "failed",
                "title": meta['title'],
                "reason": "PDFä¸‹è½½å¤±è´¥",
                "url": meta['pdf_url']
            }

        # 4. æ·±åº¦åˆ†æ (OCR + LLM) - ä½¿ç”¨ç‹¬ç«‹çš„ä¿¡å·é‡ï¼Œå®ç°çœŸæ­£æµæ°´çº¿
        deep_analysis_result = None
        if not skip_deep:
            paper_info = {
                'title': meta['title'],
                'authors': meta['authors'],
                'date': target_date,
                'arxiv_id': arxiv_id
            }

            # 4.1 OCRé˜¶æ®µ - ç‹¬ç«‹ä¿¡å·é‡ï¼Œå¸¦é‡è¯•æœºåˆ¶
            progress.update(task_id, description=f"[magenta]ğŸ“„ OCRè¯†åˆ«: {meta['title'][:30]}...", advance=15)
            ocr_result = None
            max_ocr_retries = 2

            for ocr_attempt in range(max_ocr_retries):
                async with ocr_sem:
                    ocr_result = await loop.run_in_executor(
                        None,
                        paper_analyzer.extract_ocr_only,
                        pdf_path, paper_info, category_dir
                    )

                if ocr_result is not None:
                    if ocr_attempt > 0:
                        print(f"   âœ… OCRæˆåŠŸ (ç¬¬{ocr_attempt + 1}æ¬¡å°è¯•): {meta['title'][:40]}...")
                    break
                else:
                    print(f"   âš ï¸  OCRå¤±è´¥ (å°è¯• {ocr_attempt + 1}/{max_ocr_retries}): {meta['title'][:40]}...")
                    if ocr_attempt < max_ocr_retries - 1:
                        await asyncio.sleep(2 * (ocr_attempt + 1))  # é€’å¢å»¶è¿Ÿ

            if ocr_result is None:
                print(f"   âŒ OCRé˜¶æ®µæœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡LLMåˆ†æ: {meta['title'][:40]}...")
                deep_analysis_result = None
            else:
                # 4.2 LLMåˆ†æé˜¶æ®µ - ç‹¬ç«‹ä¿¡å·é‡ï¼Œå¸¦é‡è¯•æœºåˆ¶
                progress.update(task_id, description=f"[magenta]ğŸ§  LLMåˆ†æ: {meta['title'][:30]}...", advance=15)
                deep_analysis_result = None
                max_llm_retries = 2

                for llm_attempt in range(max_llm_retries):
                    async with llm_sem:
                        deep_analysis_result = await loop.run_in_executor(
                            None,
                            paper_analyzer.analyze_with_llm,
                            ocr_result, paper_info, category_dir
                        )

                    if deep_analysis_result is not None:
                        if llm_attempt > 0:
                            print(f"   âœ… LLMåˆ†ææˆåŠŸ (ç¬¬{llm_attempt + 1}æ¬¡å°è¯•): {meta['title'][:40]}...")
                        break
                    else:
                        print(f"   âš ï¸  LLMåˆ†æå¤±è´¥ (å°è¯• {llm_attempt + 1}/{max_llm_retries}): {meta['title'][:40]}...")
                        if llm_attempt < max_llm_retries - 1:
                            await asyncio.sleep(2 * (llm_attempt + 1))
        
        # 5. ä¸Šä¼  Zotero & ç¬”è®°
        progress.update(task_id, description=f"[green]ğŸ“¤ ä¸Šä¼ ä¸­: {meta['title'][:30]}...", advance=30)
        tags = analysis.get('tags', [])
        tags.append(f"Date:{target_date}")
        
        note_content = ""
        if deep_analysis_result and deep_analysis_result.get('note_path'):
            try:
                with open(deep_analysis_result['note_path'], 'r', encoding='utf-8') as f:
                    note_content = f.read()
            except: pass
        
        if not note_content:
            note_content = await loop.run_in_executor(
                None,
                llm_agent.generate_reading_note,
                meta['title'], ", ".join(meta['authors']), meta['summary'], analysis
            )
        
        await loop.run_in_executor(
            None,
            zotero_ops.upload_paper_linked,
            meta, pdf_path, note_content, tags, category
        )
        
        progress.update(task_id, description=f"[bold green]âœ… å·²å®Œæˆ: {meta['title'][:30]}", completed=100)
        return {
            "status": "interested",
            "title": meta['title'],
            "url": meta['pdf_url'],
            "category": category,
            "summary": analysis.get('summary_cn', 'æ— æ€»ç»“'),
            "tricks": analysis.get('tricks_cn', 'æ— '),
            "reason": analysis.get('reason', ''),
            "local_path": pdf_path,
            "deep_analysis": deep_analysis_result
        }

    except Exception as e:
        error_msg = str(e)
        progress.update(task_id, description=f"[red]âŒ å¤±è´¥: {meta['title'][:30]}")
        print(f"\nâŒ è®ºæ–‡å¤„ç†å¤±è´¥: {meta['title']}")
        print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
        import traceback
        print(f"   å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
        return None

def generate_daily_report(interested, ignored, date, local_dir):
    """ç”Ÿæˆæ±‡æ€»å¼æ—¥æŠ¥ - åŸºäºLLMçš„åˆ†æ‰¹æ¬¡æ±‡æ€»"""
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆæ±‡æ€»æ—¥æŠ¥...")
    os.makedirs(local_dir, exist_ok=True)

    if not interested:
        md = [f"# AI ç§‘ç ”æƒ…æŠ¥ - {date}", "", "ä»Šæ—¥æ— æ„Ÿå…´è¶£è®ºæ–‡ã€‚"]
        report_path = os.path.join(local_dir, "00_Daily_Report_CN.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(md))
        print(f"âœ… æ—¥æŠ¥ç”Ÿæˆå®Œæ¯•: {report_path}")
        return

    # 1. æ”¶é›†æ‰€æœ‰è®ºæ–‡çš„noteå†…å®¹
    print(f"   ğŸ“š æ­£åœ¨è¯»å– {len(interested)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ç¬”è®°...")
    papers_notes = []
    for item in interested:
        if not item:
            continue
        note_path = item.get('deep_analysis', {}).get('note_path', '') if item.get('deep_analysis') else ''
        if note_path and os.path.exists(note_path):
            try:
                with open(note_path, 'r', encoding='utf-8') as f:
                    note_content = f.read()
                # è§£ænoteå†…å®¹
                parsed = llm_agent.parse_note_content(note_content)
                parsed['category'] = item.get('category', 'Uncategorized')
                parsed['url'] = item.get('url', '')
                parsed['note_path'] = note_path
                papers_notes.append(parsed)
            except Exception as e:
                print(f"   âš ï¸  è¯»å–ç¬”è®°å¤±è´¥: {note_path}, é”™è¯¯: {e}")
                continue
        else:
            # å¦‚æœæ²¡æœ‰è¯¦ç»†ç¬”è®°ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯
            deep = item.get('deep_analysis', {})
            analysis = deep.get('analysis', {}) if deep else {}
            papers_notes.append({
                'title': item.get('title', ''),
                'title_cn': analysis.get('title_cn', ''),
                'authors': analysis.get('authors', []),
                'category': item.get('category', 'Uncategorized'),
                'core_problem': analysis.get('core_problem', ''),
                'core_contribution': analysis.get('core_contribution', []),
                'method_summary': analysis.get('method_summary', ''),
                'key_results': analysis.get('key_results', ''),
                'pros': analysis.get('pros', []),
                'url': item.get('url', ''),
                'note_path': note_path
            })

    if not papers_notes:
        print("   âš ï¸  æ²¡æœ‰å¯ç”¨çš„è®ºæ–‡ç¬”è®°")
        return

    print(f"   âœ… æˆåŠŸè¯»å– {len(papers_notes)} ç¯‡è®ºæ–‡ç¬”è®°")

    # 2. åˆ†æ‰¹æ¬¡è¿›è¡Œå°æ±‡æ€»ï¼ˆæ¯10ç¯‡ä¸€æ‰¹ï¼‰
    batch_size = 10
    batches = [papers_notes[i:i+batch_size] for i in range(0, len(papers_notes), batch_size)]
    print(f"   ğŸ”„ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œæ±‡æ€»...")

    batch_summaries = []
    for batch_idx, batch in enumerate(batches):
        print(f"      ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}: {len(batch)} ç¯‡è®ºæ–‡")
        summary = llm_agent.summarize_papers_batch(batch, batch_idx)
        batch_summaries.append(summary)

    # 3. åŸºäºå°æ±‡æ€»ç”Ÿæˆæœ€ç»ˆæ—¥æŠ¥
    print(f"   ğŸ§  æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ—¥æŠ¥...")
    final_report = llm_agent.generate_final_daily_report(batch_summaries, len(papers_notes), date)

    # 4. æ„å»ºMarkdownæ—¥æŠ¥
    md = [f"# AI ç§‘ç ”æƒ…æŠ¥ - {date}", ""]

    # 4.1 ä»Šæ—¥æ¦‚è§ˆ
    md.append("## 1. ä»Šæ—¥æ¦‚è§ˆ")
    md.append(final_report.get('daily_overview', 'ä»Šæ—¥æ¦‚è§ˆç”Ÿæˆå¤±è´¥'))
    md.append("")

    # 4.2 æ ¸å¿ƒæ´å¯Ÿ
    key_insights = final_report.get('key_insights', [])
    if key_insights:
        md.append("## 2. æ ¸å¿ƒæ´å¯Ÿ")
        md.append("")
        for idx, insight in enumerate(key_insights, 1):
            md.append(f"{idx}. {insight}")
        md.append("")

    # 4.3 æ–¹å‘å°ç»“
    direction_summary = final_report.get('direction_summary', {})
    if direction_summary:
        md.append("## 3. æ–¹å‘å°ç»“")
        md.append("")
        for direction, summary in direction_summary.items():
            if summary:
                md.append(f"### {direction}")
                md.append(summary)
                md.append("")

    # 4.4 å„æ‰¹æ¬¡è¯¦ç»†æ±‡æ€»
    md.append("## 4. è®ºæ–‡è¯¦ç»†æ±‡æ€»")
    md.append("")

    for batch_idx, (batch, summary) in enumerate(zip(batches, batch_summaries), 1):
        md.append(f"### 4.{batch_idx} æ‰¹æ¬¡ {batch_idx} ({len(batch)} ç¯‡)")
        md.append("")

        # æ‰¹æ¬¡æ•´ä½“è¶‹åŠ¿
        batch_summary_text = summary.get('batch_summary', '')
        if batch_summary_text:
            md.append(f"**æ•´ä½“è¶‹åŠ¿**: {batch_summary_text}")
            md.append("")

        # æŠ€æœ¯è¶‹åŠ¿
        tech_trends = summary.get('technical_trends', [])
        if tech_trends:
            md.append(f"**æŠ€æœ¯è¶‹åŠ¿**: {', '.join(tech_trends)}")
            md.append("")

        # æ¯ç¯‡è®ºæ–‡è¦ç‚¹
        papers_highlights = summary.get('papers_highlights', [])
        md.append("**è®ºæ–‡è¦ç‚¹**:")
        md.append("")

        for paper_idx, (paper, highlight) in enumerate(zip(batch, papers_highlights), 1):
            md.append(f"{paper_idx}. **{paper.get('title', '')}**")
            if paper.get('title_cn'):
                md.append(f"   - ä¸­æ–‡æ ‡é¢˜: {paper['title_cn']}")

            # ä½¿ç”¨LLMæå–çš„è¦ç‚¹
            if highlight:
                md.append(f"   - æ–¹æ³•: {highlight.get('key_method', '')}")
                md.append(f"   - å‘ç°: {highlight.get('key_finding', '')}")
                if highlight.get('result_highlight'):
                    md.append(f"   - ç»“æœ: {highlight.get('result_highlight')}")
            else:
                # å¤‡ç”¨ï¼šä½¿ç”¨è§£æçš„å†…å®¹
                if paper.get('method_summary'):
                    method = paper['method_summary'][:100] + "..." if len(paper['method_summary']) > 100 else paper['method_summary']
                    md.append(f"   - æ–¹æ³•: {method}")

            # é“¾æ¥
            if paper.get('note_path') and os.path.exists(paper['note_path']):
                rel_note = os.path.relpath(paper['note_path'], local_dir)
                md.append(f"   - ğŸ“„ [è¯¦ç»†ç¬”è®°]({rel_note}) | ğŸ”— [arXivåŸæ–‡]({paper['url']})")

            md.append("")

        md.append("---")
        md.append("")

    # 4.5 å€¼å¾—å…³æ³¨è®ºæ–‡
    notable_papers = final_report.get('notable_papers', [])
    if notable_papers:
        md.append("## 5. å€¼å¾—å…³æ³¨è®ºæ–‡")
        md.append("")
        for idx, paper in enumerate(notable_papers, 1):
            md.append(f"{idx}. **{paper.get('title', '')}**")
            md.append(f"   - {paper.get('why_notable', '')}")
            md.append("")

    # 4.6 æœªæ¥è¶‹åŠ¿
    future_trends = final_report.get('future_trends', '')
    if future_trends:
        md.append("## 6. æœªæ¥è¶‹åŠ¿å±•æœ›")
        md.append(future_trends)
        md.append("")

    # 4.7 å¿½ç•¥çš„è®ºæ–‡
    if ignored:
        md.append(f"## 7. å…¶ä»–è®ºæ–‡ ({len(ignored)} ç¯‡)")
        md.append("")
        md.append("ä»¥ä¸‹è®ºæ–‡å› ä¸ç¬¦åˆå½“å‰ç ”ç©¶æ–¹å‘è¢«è¿‡æ»¤ï¼š")
        md.append("")
        md.append("| åºå· | æ ‡é¢˜ | è¿‡æ»¤åŸå›  |")
        md.append("|:---:|:---|:---|")
        for idx, item in enumerate(ignored[:15], 1):
            short_title = item['title'][:55] + "..." if len(item['title']) > 55 else item['title']
            reason = item.get('reason', 'æœªçŸ¥åŸå› ')
            if len(reason) > 50:
                reason = reason[:50] + "..."
            md.append(f"| {idx} | [{short_title}]({item['url']}) | {reason} |")
        md.append("")

    # 5. ä¿å­˜æ—¥æŠ¥
    report_path = os.path.join(local_dir, "00_Daily_Report_CN.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(md))
    print(f"âœ… æ—¥æŠ¥ç”Ÿæˆå®Œæ¯•: {report_path}")

async def main_async():
    parser = argparse.ArgumentParser()
    parser.add_argument('--date', type=str, help='YYYY-MM-DD', default=None)
    parser.add_argument('--skip-deep-analysis', action='store_true', help='è·³è¿‡æ·±åº¦åˆ†æ')
    args = parser.parse_args()

    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    target_date = args.date if args.date else datetime.datetime.now().strftime('%Y-%m-%d')
    base_dir = config['local_storage']['base_dir']
    local_dir = os.path.join(base_dir, target_date)
    
    concurrency_config = config.get('concurrency', {})
    # å¢åŠ å¹¶å‘åº¦ - å„é˜¶æ®µç‹¬ç«‹æ§åˆ¶
    filter_limit = concurrency_config.get('paper_workers', 4)
    download_limit = 10  # ä¸‹è½½å¯ä»¥æ›´å¤š
    ocr_limit = concurrency_config.get('ocr_workers', 4)  # OCRå¹¶å‘
    llm_limit = concurrency_config.get('llm_workers', 4)  # LLMåˆ†æå¹¶å‘

    print(f"ğŸ“… æ—¥æœŸ: {target_date} | æœ¬åœ°ç›®å½•: {local_dir}")
    print(f"âš™ï¸  å¹¶å‘é…ç½®: ç­›é€‰={filter_limit}, ä¸‹è½½={download_limit}, OCR={ocr_limit}, LLM={llm_limit}")

    global ZOTERO_STRUCTURE
    ZOTERO_STRUCTURE = zotero_ops.get_existing_structure()

    arxiv_ids = hf_scraper.get_daily_papers(target_date)
    if not arxiv_ids:
        print("ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ã€‚")
        return

    print(f"ğŸ” æŠ“å–åˆ° {len(arxiv_ids)} ç¯‡ï¼Œå¼€å§‹å¼‚æ­¥æµæ°´çº¿å¤„ç†...")

    # ä¿¡å·é‡æ§åˆ¶ - 4ä¸ªç‹¬ç«‹ä¿¡å·é‡
    semaphores = (
        asyncio.Semaphore(filter_limit),   # 0: ç­›é€‰
        asyncio.Semaphore(download_limit), # 1: ä¸‹è½½
        asyncio.Semaphore(ocr_limit),      # 2: OCR
        asyncio.Semaphore(llm_limit)       # 3: LLMåˆ†æ
    )

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeElapsedColumn(),
        console=console,
        expand=True
    ) as progress:
        
        # é¢„å…ˆåˆ›å»ºæ‰€æœ‰ä»»åŠ¡å ä½
        tasks_map = {}
        for aid in arxiv_ids:
            tid = progress.add_task(f"[grey50]ç­‰å¾…ä¸­: {aid}", total=100)
            tasks_map[aid] = tid
            
        # æµå¼è·å–å…ƒæ•°æ®å¹¶å¯åŠ¨å¤„ç†
        async_tasks = []
        meta_queue = asyncio.Queue()
        
        def fetch_meta():
            try:
                for aid, meta in utils.get_arxiv_metadata_stream(arxiv_ids):
                    asyncio.run_coroutine_threadsafe(meta_queue.put((aid, meta)), loop)
            finally:
                # æ”¾å…¥ç»“æŸæ ‡è®°
                asyncio.run_coroutine_threadsafe(meta_queue.put((None, None)), loop)
        
        loop = asyncio.get_running_loop()
        threading.Thread(target=fetch_meta, daemon=True).start()
        
        while True:
            aid, meta = await meta_queue.get()
            if aid is None: break
            
            tid = tasks_map[aid]
            t = asyncio.create_task(process_paper_async(
                aid, meta, local_dir, target_date, args.skip_deep_analysis, progress, tid, semaphores
            ))
            async_tasks.append(t)
            
        if async_tasks:
            results = await asyncio.gather(*async_tasks)
        else:
            results = []
        
    interested = [r for r in results if r and r.get('status') == 'interested']
    ignored = [r for r in results if r and r.get('status') == 'ignored']
    
    generate_daily_report(interested, ignored, target_date, local_dir)

if __name__ == "__main__":
    asyncio.run(main_async())

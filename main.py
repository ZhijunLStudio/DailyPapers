import os
import yaml
import datetime
import argparse
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from src import hf_scraper, utils, llm_agent, zotero_ops, paper_analyzer

with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

ZOTERO_STRUCTURE = {}


def process_single_paper(args):
    """å¤„ç†å•ç¯‡è®ºæ–‡"""
    arxiv_id, meta, local_dir, target_date, skip_deep = args
    
    # 1. RAG åˆ†æ
    try:
        analysis = llm_agent.analyze_paper_with_structure(
            meta['title'], meta['summary'], ZOTERO_STRUCTURE
        )
    except Exception as e:
        print(f"åˆ†æè®ºæ–‡å¤±è´¥ {arxiv_id}: {e}")
        return None

    # å¦‚æœä¸æ„Ÿå…´è¶£ï¼Œè¿”å›å¿½ç•¥æ ‡è®°
    if not analysis.get('interested'):
        return {
            "status": "ignored",
            "title": meta['title'],
            "reason": analysis.get('reason', 'No reason'),
            "url": meta['pdf_url']
        }

    # === æ„Ÿå…´è¶£çš„å¤„ç†é€»è¾‘ ===
    category = analysis.get('category', 'Uncategorized')
    category_dir = os.path.join(local_dir, category)
    if not os.path.exists(category_dir):
        os.makedirs(category_dir, exist_ok=True)
        
    short_title = utils.sanitize_filename(meta['title'])[:40]
    first_author = utils.sanitize_filename(meta['authors'][0])
    filename = f"{first_author}_{short_title}.pdf"
    pdf_path = os.path.join(category_dir, filename)
    
    # ä¸‹è½½PDF
    utils.download_pdf(arxiv_id, pdf_path)
    
    # æ·±åº¦åˆ†æ
    deep_analysis_result = None
    if not skip_deep:
        try:
            print(f"\n  ğŸ”¬ æ·±åº¦åˆ†æ: {meta['title'][:50]}...")
            paper_info = {
                'title': meta['title'],
                'authors': meta['authors'],
                'date': target_date,
                'arxiv_id': arxiv_id
            }
            deep_analysis_result = paper_analyzer.analyze_paper_deep(
                pdf_path, paper_info, category_dir
            )
        except Exception as e:
            print(f"  âš ï¸ æ·±åº¦åˆ†æå¤±è´¥: {e}")
    
    # ä¸Šä¼ Zotero
    tags = analysis.get('tags', [])
    tags.append(f"Date:{target_date}")
    
    # è¯»å–ç”Ÿæˆçš„ç¬”è®°å†…å®¹
    note_content = ""
    if deep_analysis_result and deep_analysis_result.get('note_path'):
        try:
            with open(deep_analysis_result['note_path'], 'r', encoding='utf-8') as f:
                note_content = f.read()
        except:
            pass
    
    if not note_content:
        note_content = llm_agent.generate_reading_note(
            meta['title'], ", ".join(meta['authors']), meta['summary'], analysis
        )
    
    zotero_ops.upload_paper_linked(meta, pdf_path, note_content, tags, category)

    return {
        "status": "interested",
        "title": meta['title'],
        "url": meta['pdf_url'],
        "category": category,
        "summary": analysis.get('summary_cn', 'æ— æ€»ç»“'),
        "tricks": analysis.get('tricks_cn', 'æ— '),
        "reason": analysis.get('reason', ''),
        "local_path": pdf_path,
        "deep_analysis": deep_analysis_result
    }


def generate_daily_report(interested, ignored, date, local_dir):
    """
    ç”Ÿæˆæ±‡æ€»å¼æ—¥æŠ¥ - æŒ‰ä¸»é¢˜èšåˆï¼Œæç‚¼å…³é”®ä¿¡æ¯
    """
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆæ±‡æ€»æ—¥æŠ¥...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(local_dir, exist_ok=True)
    
    if not interested:
        md = [f"# AI ç§‘ç ”æƒ…æŠ¥ - {date}", "", "ä»Šæ—¥æ— æ„Ÿå…´è¶£è®ºæ–‡ã€‚"]
        report_path = os.path.join(local_dir, "00_Daily_Report_CN.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(md))
        print(f"âœ… æ—¥æŠ¥ç”Ÿæˆå®Œæ¯•: {report_path}")
        return
    
    # æ”¶é›†æ‰€æœ‰è®ºæ–‡çš„åˆ†ææ•°æ®
    papers_data = []
    for item in interested:
        deep = item.get('deep_analysis', {})
        if deep and deep.get('analysis'):
            papers_data.append({
                'title': item['title'],
                'category': item['category'],
                'reason': item['reason'],
                'analysis': deep['analysis'],
                'selected_figures': deep.get('selected_figures', []),
                'note_path': deep.get('note_path', ''),
                'url': item['url']
            })
    
    # æŒ‰ä¸»é¢˜èšåˆ
    themes = aggregate_by_theme(papers_data)
    
    # ç”Ÿæˆæ—¥æŠ¥å†…å®¹
    md = [f"# AI ç§‘ç ”æƒ…æŠ¥ - {date}", ""]
    
    # 1. ä»Šæ—¥æ¦‚è§ˆ
    md.append("## 1. ä»Šæ—¥æ¦‚è§ˆ")
    overview = generate_overview_text(papers_data, ignored)
    md.append(overview)
    md.append("")
    
    # 2. ä¸»é¢˜èšåˆåˆ†æ
    md.append(f"## 2. ä¸»é¢˜åˆ†æ ({len(interested)} ç¯‡è®ºæ–‡)")
    md.append("")
    
    for theme_name, theme_papers in themes.items():
        md.append(f"### {theme_name}")
        md.append("")
        
        # ä¸»é¢˜æ¦‚è¿°
        theme_summary = generate_theme_summary(theme_papers)
        md.append(theme_summary)
        md.append("")
        
        # è¯¥ä¸»é¢˜ä¸‹çš„è®ºæ–‡
        for paper in theme_papers:
            md.append(f"**{paper['title']}**")
            md.append(f"")
            
            # æ ¸å¿ƒè´¡çŒ®
            contribution = paper['analysis'].get('core_contribution', '')
            if isinstance(contribution, list):
                for item in contribution[:2]:  # æœ€å¤š2ç‚¹
                    md.append(f"- {item}")
            else:
                md.append(f"- {contribution}")
            
            # å…³é”®trick
            tricks = paper['analysis'].get('key_results', '')
            if tricks and len(tricks) > 10:
                md.append(f"")
                md.append(f"ğŸ’¡ **å…³é”®å‘ç°**: {tricks[:150]}...")
            
            # é“¾æ¥
            if paper.get('note_path'):
                rel_note = os.path.relpath(paper['note_path'], local_dir)
                md.append(f"")
                md.append(f"ğŸ“„ [è¯¦ç»†ç¬”è®°]({rel_note}) | [arXiv]({paper['url']})")
            
            md.append("")
        
        md.append("---")
        md.append("")
    
    # 3. è·¨è®ºæ–‡æ´å¯Ÿ
    md.append("## 3. è·¨è®ºæ–‡æ´å¯Ÿ")
    insights = generate_cross_paper_insights(papers_data)
    md.append(insights)
    md.append("")
    
    # 4. å…³é”®å›¾è¡¨ç²¾é€‰
    key_figures = select_key_figures_for_daily(papers_data)
    if key_figures:
        md.append("## 4. å…³é”®å›¾è¡¨ç²¾é€‰")
        md.append("")
        md.append("ä»¥ä¸‹æ˜¯ä»ä»Šæ—¥è®ºæ–‡ä¸­ç²¾é€‰çš„æœ€å…·ä»£è¡¨æ€§çš„å›¾è¡¨ï¼š")
        md.append("")
        
        for fig in key_figures[:6]:  # æœ€å¤š6ä¸ª
            rel_path = os.path.relpath(fig['path'], local_dir)
            md.append(f"**{fig['paper_title'][:50]}... - {fig['desc'][:80]}**")
            md.append(f"")
            md.append(f"![{fig['desc']}]({rel_path})")
            md.append(f"")
        
        md.append("")
    
    # 5. å¿½ç•¥çš„è®ºæ–‡
    if ignored:
        md.append(f"## 5. å…¶ä»–è®ºæ–‡ ({len(ignored)} ç¯‡)")
        md.append("")
        md.append("| æ ‡é¢˜ | è¿‡æ»¤åŸå›  |")
        md.append("|---|---|")
        for item in ignored[:15]:  # æœ€å¤šæ˜¾ç¤º15ä¸ª
            short_title = item['title'][:60] + "..." if len(item['title']) > 60 else item['title']
            md.append(f"| [{short_title}]({item['url']}) | {item['reason'][:50]} |")
        md.append("")
    
    report_path = os.path.join(local_dir, "00_Daily_Report_CN.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(md))
    
    print(f"âœ… æ—¥æŠ¥ç”Ÿæˆå®Œæ¯•: {report_path}")


def aggregate_by_theme(papers_data):
    """æŒ‰ä¸»é¢˜èšåˆè®ºæ–‡"""
    themes = {}
    
    for paper in papers_data:
        category = paper['category']
        if category not in themes:
            themes[category] = []
        themes[category].append(paper)
    
    return themes


def generate_overview_text(papers_data, ignored):
    """ç”Ÿæˆä»Šæ—¥æ¦‚è§ˆæ–‡æœ¬"""
    total = len(papers_data)
    categories = set(p['category'] for p in papers_data)
    
    text = f"ä»Šæ—¥å…±ç­›é€‰å‡º **{total}** ç¯‡æ„Ÿå…´è¶£è®ºæ–‡"
    if categories:
        text += f"ï¼Œæ¶µç›– **{', '.join(categories)}** ç­‰æ–¹å‘"
    text += "ã€‚"
    
    if ignored:
        text += f"å¦æœ‰ {len(ignored)} ç¯‡è®ºæ–‡å› ä¸ç¬¦åˆç ”ç©¶æ–¹å‘è¢«è¿‡æ»¤ã€‚"
    
    return text


def generate_theme_summary(theme_papers):
    """ç”Ÿæˆä¸»é¢˜æ¦‚è¿°"""
    if len(theme_papers) == 1:
        paper = theme_papers[0]
        problem = paper['analysis'].get('core_problem', '')
        return f"è¯¥ä¸»é¢˜åŒ…å«1ç¯‡è®ºæ–‡ï¼Œä¸»è¦å…³æ³¨ï¼š{problem[:100]}..."
    else:
        # å¤šç¯‡è®ºæ–‡ï¼Œæ‰¾å…±åŒç‚¹
        problems = [p['analysis'].get('core_problem', '') for p in theme_papers]
        return f"è¯¥ä¸»é¢˜åŒ…å« {len(theme_papers)} ç¯‡è®ºæ–‡ï¼Œå…±åŒæ¢è®¨ç›¸å…³æŠ€æœ¯æ–¹å‘ã€‚"


def generate_cross_paper_insights(papers_data):
    """ç”Ÿæˆè·¨è®ºæ–‡æ´å¯Ÿ"""
    insights = []
    
    # ç»Ÿè®¡å¸¸è§æŠ€æœ¯
    all_methods = []
    for p in papers_data:
        method = p['analysis'].get('method_summary', '')
        if method:
            all_methods.append(method)
    
    if len(papers_data) >= 2:
        insights.append(f"1. ä»Šæ—¥ {len(papers_data)} ç¯‡è®ºæ–‡å‘ˆç°å‡ºå¯¹å¤šæ¨¡æ€å’ŒAgentæŠ€æœ¯çš„æŒç»­å…³æ³¨ã€‚")
        insights.append(f"2. ç ”ç©¶æ–¹æ³•ä¸Šï¼Œå„è®ºæ–‡å‡é‡‡ç”¨äº†å¤§è§„æ¨¡å®éªŒéªŒè¯å’Œå¯¹æ¯”åˆ†æã€‚")
    
    if not insights:
        return "ä»Šæ—¥è®ºæ–‡è¾ƒä¸ºåˆ†æ•£ï¼Œæš‚æ— æ˜æ˜¾çš„è·¨è®ºæ–‡è¶‹åŠ¿ã€‚"
    
    return "\n".join(insights)


def select_key_figures_for_daily(papers_data):
    """ä»æ‰€æœ‰è®ºæ–‡ä¸­é€‰æ‹©æœ€å…³é”®çš„å›¾è¡¨"""
    all_figures = []
    
    for paper in papers_data:
        for fig in paper.get('selected_figures', []):
            all_figures.append({
                'path': fig['crop_path'],
                'paper_title': paper['title'],
                'desc': fig.get('analysis_desc', fig.get('caption', 'å…³é”®å›¾è¡¨')),
                'type': fig['type']
            })
    
    # ä¼˜å…ˆé€‰æ‹©æœ‰æè¿°çš„å›¾è¡¨
    figures_with_desc = [f for f in all_figures if f['desc'] and len(f['desc']) > 10]
    
    # æ··åˆé€‰æ‹©ï¼šæ¶æ„å›¾ã€ç»“æœå›¾ã€è¡¨æ ¼
    selected = []
    types_needed = {'figure': 2, 'table': 1}
    
    for fig_type, count in types_needed.items():
        type_figs = [f for f in figures_with_desc if f['type'] == fig_type or 
                     (fig_type == 'figure' and f['type'] in ['image', 'figure'])]
        selected.extend(type_figs[:count])
    
    # è¡¥å……å…¶ä»–
    remaining = [f for f in figures_with_desc if f not in selected]
    selected.extend(remaining[:3])
    
    return selected[:6]


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--date', type=str, help='YYYY-MM-DD', default=None)
    parser.add_argument('--workers', type=int, default=None, 
                        help='è®ºæ–‡å¤„ç†å¹¶å‘æ•°ï¼ˆé»˜è®¤ä»configè¯»å–ï¼‰')
    parser.add_argument('--skip-deep-analysis', action='store_true', 
                        help='è·³è¿‡æ·±åº¦åˆ†æ')
    args = parser.parse_args()

    target_date = args.date if args.date else datetime.datetime.now().strftime('%Y-%m-%d')
    base_dir = config['local_storage']['base_dir']
    local_dir = os.path.join(base_dir, target_date)
    
    # ä»é…ç½®è¯»å–å¹¶å‘æ•°
    concurrency_config = config.get('concurrency', {})
    paper_workers = args.workers or concurrency_config.get('paper_workers', 2)
    arxiv_chunk_size = concurrency_config.get('arxiv_chunk_size', 10)
    arxiv_delay = concurrency_config.get('arxiv_delay', 3)
    
    print(f"ğŸ“… æ—¥æœŸ: {target_date} | æœ¬åœ°ç›®å½•: {local_dir}")
    print(f"âš™ï¸  å¹¶å‘é…ç½®: è®ºæ–‡å¤„ç†={paper_workers}ç¯‡, arXivæ‰¹æ¬¡={arxiv_chunk_size}")
    
    global ZOTERO_STRUCTURE
    ZOTERO_STRUCTURE = zotero_ops.get_existing_structure()
    
    arxiv_ids = hf_scraper.get_daily_papers(target_date)
    if not arxiv_ids:
        print("ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ã€‚")
        return

    print(f"ğŸ” æŠ“å–åˆ° {len(arxiv_ids)} ç¯‡ï¼Œè·å–å…ƒæ•°æ®...")
    papers_meta = utils.get_arxiv_metadata(arxiv_ids, chunk_size=arxiv_chunk_size, delay=arxiv_delay)
    
    print("ğŸš€ å¼€å§‹ AI å¤„ç†...")
    
    tasks = []
    for arxiv_id, meta in papers_meta.items():
        tasks.append((arxiv_id, meta, local_dir, target_date, args.skip_deep_analysis))

    interested_results = []
    ignored_results = []

    with ThreadPoolExecutor(max_workers=paper_workers) as executor:
        future_to_paper = {executor.submit(process_single_paper, task): task for task in tasks}
        for future in tqdm(as_completed(future_to_paper), total=len(tasks)):
            try:
                res = future.result()
                if res:
                    if res['status'] == 'interested':
                        interested_results.append(res)
                    else:
                        ignored_results.append(res)
            except Exception as exc:
                print(f"å¼‚å¸¸: {exc}")

    generate_daily_report(interested_results, ignored_results, target_date, local_dir)


if __name__ == "__main__":
    main()

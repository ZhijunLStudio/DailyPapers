"""
è®ºæ–‡æ·±åº¦åˆ†ææ¨¡å— - å¯¹æ„Ÿå…´è¶£çš„è®ºæ–‡è¿›è¡ŒPDFè½¬å›¾ç‰‡ã€OCRåˆ†æã€å›¾æ–‡æŠ¥å‘Šç”Ÿæˆ
æ”¯æŒå¹¶å‘OCRå¤„ç†
"""
import os
import re
import base64
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
from PIL import Image, ImageDraw, ImageFont
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import yaml

with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# è·å–é…ç½®
analysis_config = config.get('analysis', {})
concurrency_config = config.get('concurrency', {})
ocr_config = config.get('deepseek_ocr', {})
openai_config = config.get('openai', {})

# OpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºå†…å®¹åˆ†æï¼‰
client = OpenAI(
    api_key=config['openai']['api_key'],
    base_url=config['openai']['base_url']
)

# DeepSeek OCRå®¢æˆ·ç«¯
if ocr_config.get('api_key') and ocr_config.get('base_url'):
    ocr_client = OpenAI(
        api_key=ocr_config['api_key'],
        base_url=ocr_config['base_url']
    )
else:
    ocr_client = client

# Tokenæ¶ˆè€—è®°å½•
token_usage = {
    'ocr_calls': 0,
    'ocr_tokens': 0,
    'llm_calls': 0,
    'llm_tokens_input': 0,
    'llm_tokens_output': 0
}


def pdf_to_images(pdf_path: str, output_dir: str, dpi: int = None) -> List[str]:
    """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡ï¼Œå®æ—¶æ‰“å°è¿›åº¦"""
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… PyMuPDF: pip install PyMuPDF")
        return []

    dpi = dpi or analysis_config.get('pdf_dpi', 200)
    os.makedirs(output_dir, exist_ok=True)

    # å°è¯•ç¦ç”¨MuPDFçš„è­¦å‘Šè¾“å‡ºï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
    try:
        fitz.set_messages_enabled(False)
    except AttributeError:
        pass  # æŸäº›PyMuPDFç‰ˆæœ¬ä¸æ”¯æŒæ­¤æ–¹æ³•

    doc = None
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        image_paths = []
        failed_pages = []

        for page_num in range(total_pages):
            try:
                page = doc[page_num]
                mat = fitz.Matrix(dpi/72, dpi/72)
                pix = page.get_pixmap(matrix=mat)
                image_path = os.path.join(output_dir, f"page_{page_num+1:03d}.png")
                pix.save(image_path)
                image_paths.append(image_path)
            except Exception as e:
                failed_pages.append(page_num + 1)
                print(f"   âš ï¸  Page {page_num + 1} è½¬æ¢å¤±è´¥: {str(e)[:50]}")
                continue

        if failed_pages:
            print(f"   âš ï¸  PDFè½¬æ¢: {len(failed_pages)}/{total_pages} é¡µå¤±è´¥ (é¡µç : {failed_pages[:5]}{'...' if len(failed_pages) > 5 else ''})")

        return image_paths
    except Exception as e:
        print(f"   âŒ PDFæ‰“å¼€å¤±è´¥: {str(e)[:100]}")
        return []
    finally:
        if doc:
            try:
                doc.close()
            except:
                pass


def call_deepseek_ocr(image_path: str) -> Tuple[str, Dict]:
    """è°ƒç”¨DeepSeek-OCRæ¨¡å‹åˆ†æå›¾ç‰‡"""
    global token_usage
    
    timeout = ocr_config.get('timeout', 120)
    max_retries = ocr_config.get('max_retries', 3)
    retry_delay = ocr_config.get('retry_delay', 5)
    
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_image}"},
                    "detail": "high"
                },
                {
                    "type": "text",
                    "text": "<|grounding|>Convert the document to markdown."
                }
            ]
        }
    ]
    
    for attempt in range(max_retries):
        try:
            ocr_model = ocr_config.get('model', 'deepseek-ocr')
            start_time = time.time()
            response = ocr_client.chat.completions.create(
                model=ocr_model,
                messages=messages,
                temperature=0.1,
                timeout=timeout
            )
            elapsed = time.time() - start_time
            
            token_usage['ocr_calls'] += 1
            usage = response.usage
            if usage:
                token_usage['ocr_tokens'] += usage.total_tokens
            
            return response.choices[0].message.content, {
                'model': ocr_model,
                'elapsed': elapsed,
                'tokens': usage.total_tokens if usage else 0
            }
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  OCRè°ƒç”¨å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡å°è¯•): {error_msg}")
            if attempt < max_retries - 1:
                print(f"   ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                print(f"âŒ OCRè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                return None, {}


def process_single_page(args):
    """å¤„ç†å•é¡µï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    page_idx, image_path, ocr_dir, figures_dir, save_viz, save_cropped = args
    page_num = page_idx + 1
    
    # OCRè¯†åˆ«
    ocr_text, ocr_token_info = call_deepseek_ocr(image_path)
    if not ocr_text:
        print(f"   ğŸ“„ Page {page_num}: OCRè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µ")
        return None

    try:
        ocr_items = parse_ocr_response(ocr_text)
    except Exception as e:
        print(f"   ğŸ“„ Page {page_num}: OCRç»“æœè§£æå¤±è´¥ - {str(e)}")
        return None
    
    # ä¿å­˜OCRæ–‡æœ¬
    ocr_txt_path = os.path.join(ocr_dir, f"page_{page_num:03d}.txt")
    with open(ocr_txt_path, 'w', encoding='utf-8') as f:
        f.write(ocr_text)
    
    # å¯è§†åŒ–
    if save_viz:
        try:
            vis_path = os.path.join(ocr_dir, f"page_{page_num:03d}_vis.png")
            visualize_ocr_result(image_path, ocr_items, vis_path)
        except Exception as e:
            print(f"   ğŸ“„ Page {page_num}: å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ - {str(e)}")

    # æå–å…³é”®å›¾è¡¨
    page_figures = []
    if save_cropped:
        try:
            page_figures = extract_key_figures(ocr_items, image_path, figures_dir, page_num)
        except Exception as e:
            print(f"   ğŸ“„ Page {page_num}: å›¾è¡¨æå–å¤±è´¥ - {str(e)}")
    
    return {
        'page': page_num,
        'items': ocr_items,
        'raw_text': ocr_text,
        'figures': page_figures
    }


def parse_ocr_response(content: str) -> List[Dict[str, Any]]:
    """è§£æOCRå“åº”ï¼Œæå–å„ä¸ªåŒºåŸŸ"""
    items = []
    tag_pattern = re.compile(r'(?P<type>\w+)\[\[(?P<rect>[\d,\s,]+)\]\]')
    matches = list(tag_pattern.finditer(content))
    
    for i, match in enumerate(matches):
        data = match.groupdict()
        label = data['type']
        rect_str = data['rect']
        
        try:
            bbox = [int(x) for x in re.split(r'[,\s]+', rect_str.strip()) if x]
        except ValueError:
            continue
            
        start_idx = match.end()
        if i < len(matches) - 1:
            end_idx = matches[i+1].start()
        else:
            end_idx = len(content)
            
        text_content = content[start_idx:end_idx].strip()
        
        items.append({
            "type": label,
            "bbox": bbox,
            "text": text_content
        })
    
    return items


def crop_region(image_path: str, bbox: List[int], output_path: str):
    """ä»å›¾ç‰‡ä¸­è£å‰ªæŒ‡å®šåŒºåŸŸ"""
    img = Image.open(image_path)
    width, height = img.size
    
    x1 = int(bbox[0] / 1000 * width)
    y1 = int(bbox[1] / 1000 * height)
    x2 = int(bbox[2] / 1000 * width)
    y2 = int(bbox[3] / 1000 * height)
    
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(width, x2), min(height, y2)
    
    if x2 > x1 and y2 > y1:
        cropped = img.crop((x1, y1, x2, y2))
        cropped.save(output_path)


def visualize_ocr_result(image_path: str, ocr_items: List[Dict], output_path: str):
    """ç»˜åˆ¶OCRå¯è§†åŒ–ç»“æœ - åªåœ¨åŸå›¾ä¸Šç”»æ¡†å’Œæ ‡ç­¾"""
    img = Image.open(image_path).convert("RGB")
    width, height = img.size
    draw = ImageDraw.Draw(img)
    
    try:
        font_size = max(12, int(width / 80))
        font = ImageFont.truetype("/System/Library/Fonts/Supplemental/Arial.ttf", size=font_size)
    except IOError:
        font = ImageFont.load_default()
    
    color_map = {
        "title": (255, 0, 0),
        "text": (0, 0, 0),
        "header": (0, 128, 0),
        "figure": (0, 0, 255),
        "image": (0, 0, 255),
        "image_caption": (255, 165, 0),
        "caption": (255, 165, 0),
        "table": (128, 0, 128),
        "table_caption": (255, 105, 180),
        "sub_title": (0, 128, 128),
        "author": (128, 128, 0),
        "abstract": (70, 130, 180),
        "reference": (105, 105, 105),
        "formula": (255, 20, 147),
        "code": (0, 100, 0),
    }
    
    for item in ocr_items:
        bbox = item['bbox']
        label = item['type']
        
        if len(bbox) == 4:
            x1 = int(bbox[0] / 1000 * width)
            y1 = int(bbox[1] / 1000 * height)
            x2 = int(bbox[2] / 1000 * width)
            y2 = int(bbox[3] / 1000 * height)
        else:
            continue
        
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(width, x2), min(height, y2)
        
        color = color_map.get(label, (100, 100, 100))
        
        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
        text_w = font.getlength(label)
        draw.rectangle([x1, y1 - font_size - 2, x1 + text_w + 4, y1], fill=color)
        draw.text((x1 + 2, y1 - font_size), label, fill=(255, 255, 255), font=font)
    
    img.save(output_path)


def extract_key_figures(ocr_items: List[Dict], image_path: str, 
                        figures_dir: str, page_num: int) -> List[Dict]:
    """æå–å…³é”®å›¾è¡¨"""
    os.makedirs(figures_dir, exist_ok=True)
    
    key_figures = []
    
    for i, item in enumerate(ocr_items):
        label = item['type']
        
        if label not in ['image', 'figure', 'table']:
            continue
        
        caption = ""
        for j in range(i+1, min(i+3, len(ocr_items))):
            if ocr_items[j]['type'] in ['caption', 'image_caption', 'table_caption']:
                caption = ocr_items[j]['text']
                break
        
        # æ¸…ç†captionä¸­çš„HTMLæ ‡ç­¾
        import re
        caption = re.sub(r'<[^>]+>', '', caption)
        caption = re.sub(r'\s+', ' ', caption).strip()
        
        ext = 'fig' if label in ['image', 'figure'] else 'table'
        crop_path = os.path.join(figures_dir, f"{ext}_p{page_num:03d}_{i+1:02d}.png")
        crop_region(image_path, item['bbox'], crop_path)
        
        key_figures.append({
            'type': label,
            'page': page_num,
            'index': i + 1,
            'caption': caption,
            'bbox': item['bbox'],
            'crop_path': crop_path,
            'text': item.get('text', '')
        })
    
    return key_figures


def analyze_paper_content(ocr_results: List[Dict]) -> Tuple[Dict, Dict]:
    """ä½¿ç”¨LLMåˆ†æè®ºæ–‡OCRå†…å®¹"""
    global token_usage
    
    timeout = openai_config.get('timeout', 60)
    max_retries = openai_config.get('max_retries', 3)
    retry_delay = openai_config.get('retry_delay', 5)
    max_length = analysis_config.get('max_ocr_text_length', 12000)
    
    # åˆå¹¶æ–‡æœ¬
    all_text = ""
    for page in ocr_results:
        page_num = page['page']
        all_text += f"\n\n=== Page {page_num} ===\n\n"
        for item in page['items']:
            all_text += f"[{item['type']}] {item['text']}\n"
    
    if len(all_text) > max_length:
        all_text = all_text[:max_length] + "\n... (å†…å®¹å·²æˆªæ–­)"
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹è®ºæ–‡çš„OCRå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

è®ºæ–‡å†…å®¹:
{all_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›:
{{
    "title": "è®ºæ–‡æ ‡é¢˜",
    "title_cn": "è®ºæ–‡ä¸­æ–‡æ ‡é¢˜æˆ–ç¿»è¯‘",
    "authors": ["ä½œè€…1", "ä½œè€…2"],
    "abstract": "æ‘˜è¦å†…å®¹",
    "core_problem": "æ ¸å¿ƒé—®é¢˜æè¿°ï¼Œç”¨1-2å¥è¯æ¦‚æ‹¬",
    "core_contribution": ["æ ¸å¿ƒè´¡çŒ®1", "æ ¸å¿ƒè´¡çŒ®2", "æ ¸å¿ƒè´¡çŒ®3"],
    "method_summary": "æ–¹æ³•æ¦‚è¿°ï¼ˆè¯¦ç»†ç‰ˆï¼Œ300-500å­—ï¼‰ï¼š
        - æ•´ä½“æ€è·¯ï¼šè®ºæ–‡è§£å†³é—®é¢˜çš„æ ¸å¿ƒæ€è·¯æ˜¯ä»€ä¹ˆ
        - å…³é”®æŠ€æœ¯ï¼šå…·ä½“ä½¿ç”¨äº†å“ªäº›æŠ€æœ¯æˆ–æ–¹æ³•
        - åˆ›æ–°ç‚¹ï¼šç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰ä»€ä¹ˆåˆ›æ–°
        - å®ç°ç»†èŠ‚ï¼šå…³é”®çš„å®ç°ç»†èŠ‚æˆ–ç®—æ³•æµç¨‹",
    "key_figures_description": [
        "å›¾1: æ¶æ„å›¾/æ¦‚è§ˆå›¾ - ç”¨ä¸­æ–‡æè¿°è¿™å¼ å›¾å±•ç¤ºäº†ä»€ä¹ˆæ¡†æ¶æˆ–ç³»ç»Ÿ",
        "å›¾2: å®éªŒç»“æœå›¾ - ç”¨ä¸­æ–‡æè¿°è¿™å¼ å›¾å±•ç¤ºäº†ä»€ä¹ˆå®éªŒç»“æœæˆ–å¯¹æ¯”"
    ],
    "key_results": "ä¸»è¦å®éªŒç»“æœï¼ˆ200-300å­—ï¼‰ï¼šåœ¨å“ªäº›æ•°æ®é›†ä¸Šåšäº†å®éªŒï¼Œæ€§èƒ½å¦‚ä½•æå‡ï¼Œä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”",
    "key_tables": ["è¡¨1: æè¿°è¡¨æ ¼å†…å®¹å’Œå…³é”®æ•°æ®"],
    "conclusion": "ç»“è®ºï¼ˆ100-200å­—ï¼‰ï¼šè®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œæ„ä¹‰",
    "pros": ["è®ºæ–‡äº®ç‚¹1", "è®ºæ–‡äº®ç‚¹2"],
    "cons": ["å±€é™æ€§1", "å±€é™æ€§2"],
    "inspirations": ["å¯¹æœªæ¥ç ”ç©¶æˆ–å®è·µçš„å¯å‘1", "å¯å‘2"]
}}

æ³¨æ„ï¼š
1. è¿”å›å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
2. æ‰€æœ‰æè¿°å¿…é¡»ä½¿ç”¨çº¯ä¸­æ–‡ï¼Œä¸¥ç¦å¤¹æ‚ä»»ä½•è‹±æ–‡åŸæ–‡ï¼ˆä¸“æœ‰åè¯é™¤å¤–ï¼‰
3. method_summaryè¦è¯¦ç»†ï¼Œåˆ†ç‚¹è¯´æ˜æ•´ä½“æ€è·¯ã€å…³é”®æŠ€æœ¯ã€åˆ›æ–°ç‚¹ã€å®ç°ç»†èŠ‚
4. ä¸¥ç¦å‡ºç°ä»»ä½•è‹±æ–‡åŸæ–‡æ®µè½ã€‚å¦‚æœ OCR ç»“æœæ˜¯è‹±æ–‡ï¼Œä½ å¿…é¡»å°†å…¶ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿ç•™æˆ–ç®€å•æ›¿æ¢ã€‚
5. key_figures_description ä¸­çš„æè¿°è¦ç”¨ä¸­æ–‡é‡æ–°ç»„ç»‡ï¼Œä¸è¦åŒ…å«ä»»ä½•å¦‚ "Figure 1 shows..." è¿™ç§è‹±æ–‡è¡¨è¾¾ï¼Œç›´æ¥æè¿°å†…å®¹ã€‚
6. å¯¹å›¾è¡¨çš„æè¿°è¦è¯¦ç»†ï¼Œè¯´æ˜å…¶ç”¨é€”å’Œå±•ç¤ºçš„å†…å®¹
7. pros, cons, inspirations å¿…é¡»æ ¹æ®è®ºæ–‡å†…å®¹ç»™å‡ºå…·ä½“çš„åˆ†æï¼Œä¸è¦ç•™ç©ºï¼Œä¸”å¿…é¡»ä¸ºä¸­æ–‡ã€‚
"""
    
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            response = client.chat.completions.create(
                model=config['openai']['model'],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=timeout
            )
            elapsed = time.time() - start_time
            
            token_usage['llm_calls'] += 1
            usage = response.usage
            if usage:
                token_usage['llm_tokens_input'] += usage.prompt_tokens
                token_usage['llm_tokens_output'] += usage.completion_tokens
            
            result = json.loads(response.choices[0].message.content)
            token_info = {
                'model': config['openai']['model'],
                'elapsed': elapsed,
                'tokens_input': usage.prompt_tokens if usage else 0,
                'tokens_output': usage.completion_tokens if usage else 0
            }
            return result, token_info
        except Exception as e:
            if attempt == max_retries - 1:
                return {}, {}
            time.sleep(retry_delay)


def select_key_figures_for_report(all_figures: List[Dict], analysis: Dict) -> List[Dict]:
    """é€‰æ‹©æœ€å…³é”®çš„å›¾è¡¨ - æ™ºèƒ½åˆ†ç±»é€‰æ‹©"""
    if not all_figures:
        return []
    
    max_figures = analysis_config.get('max_figures_per_paper', 4)
    selected = []
    
    # åˆ†ç±»ï¼šæ¶æ„å›¾ã€ç»“æœå›¾ã€è¡¨æ ¼
    arch_figures = []
    result_figures = []
    tables = []
    
    for fig in all_figures:
        caption = fig.get('caption', '').lower()
        fig_type = fig['type']
        
        # æ¶æ„å›¾å…³é”®è¯
        arch_keywords = ['arch', 'framework', 'overview', 'model', 'structure', 'pipeline', 'system', 'design']
        # ç»“æœå›¾å…³é”®è¯
        result_keywords = ['result', 'performance', 'comparison', 'ablation', 'accuracy', 'loss', 'curve', 'plot']
        
        if fig_type == 'table':
            tables.append(fig)
        elif any(kw in caption for kw in arch_keywords):
            arch_figures.append(fig)
        elif any(kw in caption for kw in result_keywords):
            result_figures.append(fig)
        else:
            # å…¶ä»–å›¾ç‰‡ï¼Œå½’å…¥ç»“æœå›¾
            result_figures.append(fig)
    
    # é€‰æ‹©ï¼š1-2å¼ æ¶æ„å›¾ï¼Œ1-2å¼ ç»“æœå›¾ï¼Œ1å¼ è¡¨æ ¼
    # å¦‚æœæŸç±»æ²¡æœ‰ï¼Œä»å…¶ä»–ç±»è¡¥å……
    selected.extend(arch_figures[:2])
    selected.extend(result_figures[:2])
    selected.extend(tables[:1])
    
    # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……å…¶ä»–å›¾
    remaining = [f for f in all_figures if f not in selected]
    selected.extend(remaining[:max_figures - len(selected)])
    
    # æ·»åŠ LLMåˆ†ææè¿° - æŒ‰é¡ºåºåŒ¹é…
    figure_descriptions = analysis.get('key_figures_description', [])
    for i, fig in enumerate(selected):
        if i < len(figure_descriptions):
            fig['analysis_desc'] = figure_descriptions[i]
    
    return selected[:max_figures]


def get_clean_caption(caption: str, fig_type: str) -> str: 
    """ä»åŸå§‹ caption ä¸­æå–ç¼–å·å¹¶è½¬æ¢ä¸º 'åŸæ–‡å›¾/è¡¨ X' æ ¼å¼ï¼Œå»é™¤å…¶ä½™è‹±æ–‡"""
    if not caption:
        return "åŸæ–‡å›¾" if fig_type != 'table' else "åŸæ–‡è¡¨"
    
    import re
    # åŒ¹é… Figure 1, Fig. 1, Table 1 ç­‰
    match = re.search(r'(?:Figure|Fig\.|Table|Tab\.)\s*(\d+[a-z]?)', caption, re.IGNORECASE)
    if match:
        num = match.group(1)
        prefix = "åŸæ–‡è¡¨" if "tab" in caption.lower() else "åŸæ–‡å›¾"
        return f"{prefix}{num}"
    
    return "åŸæ–‡å›¾" if fig_type != 'table' else "åŸæ–‡è¡¨"

def generate_paper_note(paper_info: Dict, analysis: Dict, selected_figures: List[Dict],
                        output_path: str, token_info: Dict):
    """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„è¯¦ç»†ç¬”è®° - å›¾æ–‡å¹¶èŒ‚ï¼Œå›¾è¡¨èå…¥å†…å®¹"""
    title = analysis.get('title', paper_info['title'])
    title_cn = analysis.get('title_cn', '')
    
    md_content = f"# {title}\n\n"
    
    if title_cn:
        md_content += f"**ä¸­æ–‡æ ‡é¢˜**: {title_cn}\n\n"
    
    md_content += f"**ä½œè€…**: {', '.join(analysis.get('authors', paper_info['authors']))}\n\n"
    md_content += f"**æ¥æº**: arXiv | **æ—¥æœŸ**: {paper_info.get('date', '')}\n\n"
    md_content += "---\n\n"
    
    md_content += "## æ ¸å¿ƒé—®é¢˜\n\n"
    md_content += f"{analysis.get('core_problem', 'æœªæå–')}\n\n"
    
    md_content += "## æ ¸å¿ƒè´¡çŒ®\n\n"
    contribution = analysis.get('core_contribution', '')
    if isinstance(contribution, list):
        for item in contribution:
            md_content += f"- {item}\n"
    else:
        md_content += f"{contribution}\n"
    md_content += "\n"
    
    md_content += "## æ–¹æ³•æ¦‚è¿°\n\n"
    method_summary = analysis.get('method_summary', 'æœªæå–')
    md_content += f"{method_summary}\n\n"
    
    # è®°å½•å·²ä½¿ç”¨çš„å›¾ç‰‡ï¼Œé˜²æ­¢é‡å¤
    used_figures = set()
    
    # èå…¥æ¶æ„å›¾ï¼ˆåœ¨æ–¹æ³•æ¦‚è¿°åï¼‰
    arch_figures = [f for f in selected_figures if f['type'] in ['image', 'figure'] and 
                   any(kw in f.get('caption', '').lower() for kw in ['arch', 'framework', 'overview', 'model', 'structure', 'pipeline', 'system'])]
    if arch_figures:
        md_content += "**æ¶æ„å›¾**\n\n"
        for fig in arch_figures[:2]:
            abs_path = os.path.abspath(fig['crop_path'])
            if abs_path in used_figures:
                continue
            
            desc = fig.get('analysis_desc', '')
            clean_cap = get_clean_caption(fig.get('caption', ''), fig['type'])
            
            if desc:
                md_content += f"{desc}\n\n"
            md_content += f"![{clean_cap}](file://{abs_path})\n\n"
            md_content += f"*{clean_cap}*\n\n"
            used_figures.add(abs_path)
    
    md_content += "---\n\n"
    
    md_content += "## å®éªŒç»“æœ\n\n"
    md_content += f"{analysis.get('key_results', 'æœªæå–')}\n\n"
    
    # èå…¥ç»“æœå›¾å’Œè¡¨æ ¼
    result_figures = [f for f in selected_figures if 
                     (f['type'] in ['image', 'figure'] and any(kw in f.get('caption', '').lower() for kw in ['result', 'performance', 'comparison', 'ablation', 'accuracy', 'loss', 'curve', 'plot'])) or
                     f['type'] == 'table']
    
    if result_figures:
        md_content += "**å®éªŒæ•°æ®**\n\n"
        for fig in result_figures[:3]:
            abs_path = os.path.abspath(fig['crop_path'])
            if abs_path in used_figures:
                continue
                
            desc = fig.get('analysis_desc', '')
            clean_cap = get_clean_caption(fig.get('caption', ''), fig['type'])
            
            if desc:
                md_content += f"{desc}\n\n"
            
            md_content += f"![{clean_cap}](file://{abs_path})\n\n"
            md_content += f"*{clean_cap}*\n\n"
            used_figures.add(abs_path)
    
    md_content += "---\n\n"
    md_content += "## ç»“è®º\n\n"
    md_content += f"{analysis.get('conclusion', 'æœªæå–')}\n\n"
    
    md_content += "---\n\n"
    md_content += "## ä¸ªäººæ€è€ƒ\n\n"
    md_content += "### äº®ç‚¹\n\n"
    pros = analysis.get('pros', [])
    if isinstance(pros, list):
        for item in pros:
            md_content += f"- {item}\n"
    else:
        md_content += f"{pros}\n"
    md_content += "\n"
    
    md_content += "### å±€é™æ€§\n\n"
    cons = analysis.get('cons', [])
    if isinstance(cons, list):
        for item in cons:
            md_content += f"- {item}\n"
    else:
        md_content += f"{cons}\n"
    md_content += "\n"
    
    md_content += "### å¯å‘\n\n"
    inspirations = analysis.get('inspirations', [])
    if isinstance(inspirations, list):
        for item in inspirations:
            md_content += f"- {item}\n"
    else:
        md_content += f"{inspirations}\n"
    md_content += "\n"
    
    md_content += "---\n\n"
    md_content += "## å¤„ç†è®°å½•\n\n"
    md_content += f"- OCRæ¨¡å‹: {token_info.get('ocr_model', 'unknown')}\n"
    md_content += f"- OCRè°ƒç”¨æ¬¡æ•°: {token_info.get('ocr_calls', 0)}\n"
    md_content += f"- OCRæ€»tokens: {token_info.get('ocr_tokens', 0)}\n"
    md_content += f"- LLMæ¨¡å‹: {token_info.get('llm_model', 'unknown')}\n"
    md_content += f"- LLMè°ƒç”¨æ¬¡æ•°: {token_info.get('llm_calls', 0)}\n"
    md_content += f"- LLMè¾“å…¥tokens: {token_info.get('llm_tokens_input', 0)}\n"
    md_content += f"- LLMè¾“å‡ºtokens: {token_info.get('llm_tokens_output', 0)}\n"
    md_content += f"- å¤„ç†æ—¶é—´: {token_info.get('total_time', 0):.2f}ç§’\n"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    return md_content


def analyze_paper_deep(pdf_path: str, paper_info: Dict, category_dir: str) -> Dict[str, Any]:
    """
    å¯¹è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æçš„ä¸»å‡½æ•° - æ”¯æŒå¹¶å‘OCR
    
    ç›®å½•ç»“æ„:
    Category/
    â””â”€â”€ Author_Title/
        â”œâ”€â”€ paper.pdf
        â”œâ”€â”€ note.md
        â”œâ”€â”€ ocr/
        â”‚   â”œâ”€â”€ page_001.txt
        â”‚   â””â”€â”€ page_001_vis.png
        â”œâ”€â”€ figures/
        â”‚   â”œâ”€â”€ fig_p001_01.png
        â”‚   â””â”€â”€ table_p001_02.png
        â””â”€â”€ analysis.json
    """
    global token_usage
    token_usage = {
        'ocr_calls': 0,
        'ocr_tokens': 0,
        'llm_calls': 0,
        'llm_tokens_input': 0,
        'llm_tokens_output': 0
    }
    start_total = time.time()
    
    # è¯»å–é…ç½®
    pdf_dpi = analysis_config.get('pdf_dpi', 200)
    max_pages = analysis_config.get('max_pages', 15)
    save_viz = analysis_config.get('save_visualization', True)
    save_cropped = analysis_config.get('save_cropped_figures', True)
    ocr_workers = concurrency_config.get('ocr_workers', 3)
    
    # åˆ›å»ºè®ºæ–‡ä¸“å±ç›®å½•
    paper_name = os.path.splitext(os.path.basename(pdf_path))[0]
    paper_dir = os.path.join(category_dir, paper_name)
    
    ocr_dir = os.path.join(paper_dir, "ocr")
    figures_dir = os.path.join(paper_dir, "figures")
    
    os.makedirs(ocr_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)
    
    # 1. PDFè½¬å›¾ç‰‡ï¼ˆä¸´æ—¶ç›®å½•ï¼‰
    import tempfile
    print(f"ğŸ”¬ æ·±åº¦åˆ†æ: {paper_name}")
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"   ğŸ“‘ æ­£åœ¨è½¬æ¢PDFä¸ºå›¾ç‰‡ (DPI={pdf_dpi})...")
        image_paths = pdf_to_images(pdf_path, temp_dir, dpi=pdf_dpi)
        if not image_paths:
            print(f"   âŒ PDFè½¬æ¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
            return None
        print(f"   âœ… PDFè½¬æ¢å®Œæˆï¼Œå…± {len(image_paths)} é¡µ")
        
        if max_pages is not None and len(image_paths) > max_pages:
            image_paths = image_paths[:max_pages]
        
        # 2. å¹¶å‘OCRåˆ†æ
        ocr_results = []
        all_key_figures = []

        print(f"   ğŸ” å¼€å§‹OCRåˆ†æ (å¹¶å‘æ•°={ocr_workers})...")
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = [(i, img_path, ocr_dir, figures_dir, save_viz, save_cropped)
                 for i, img_path in enumerate(image_paths)]

        # å¹¶å‘æ‰§è¡ŒOCR
        with ThreadPoolExecutor(max_workers=ocr_workers) as executor:
            futures = {executor.submit(process_single_page, task): task for task in tasks}
            for future in as_completed(futures):
                result = future.result()
                if result and isinstance(result, dict):
                    ocr_results.append(result)
                    all_key_figures.extend(result.get('figures', []))

        # æŒ‰é¡µç æ’åº
        ocr_results.sort(key=lambda x: x['page'])
        print(f"   âœ… OCRå®Œæˆ: {len(ocr_results)}/{len(image_paths)} é¡µæˆåŠŸè¯†åˆ«")

    # 3. LLMåˆ†æå†…å®¹
    print(f"   ğŸ§  æ­£åœ¨LLMåˆ†æè®ºæ–‡å†…å®¹...")
    analysis, llm_token_info = analyze_paper_content(ocr_results)
    
    # 4. é€‰æ‹©å…³é”®å›¾è¡¨
    selected_figures = select_key_figures_for_report(all_key_figures, analysis)
    
    # 5. ç”Ÿæˆè¯¦ç»†ç¬”è®°
    note_path = os.path.join(paper_dir, "note.md")
    
    token_info_summary = {
        'ocr_model': ocr_config.get('model', 'deepseek-ocr'),
        'ocr_calls': token_usage['ocr_calls'],
        'ocr_tokens': token_usage['ocr_tokens'],
        'llm_model': config['openai']['model'],
        'llm_calls': token_usage['llm_calls'],
        'llm_tokens_input': token_usage['llm_tokens_input'],
        'llm_tokens_output': token_usage['llm_tokens_output'],
        'total_time': time.time() - start_total
    }
    
    generate_paper_note(paper_info, analysis, selected_figures, note_path, token_info_summary)
    print(f"   âœ… ç¬”è®°å·²ç”Ÿæˆ: {note_path}")

    # 6. ä¿å­˜åˆ†ææ•°æ®
    analysis_data_path = os.path.join(paper_dir, "analysis.json")
    with open(analysis_data_path, 'w', encoding='utf-8') as f:
        json.dump({
            'paper_info': paper_info,
            'analysis': analysis,
            'selected_figures': selected_figures,
            'all_figures_count': len(all_key_figures),
            'token_usage': token_info_summary
        }, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ“Š åˆ†æå®Œæˆ: æå– {len(selected_figures)} å¼ å›¾è¡¨, è€—æ—¶ {token_info_summary['total_time']:.1f}ç§’")

    return {
        'paper_dir': paper_dir,
        'note_path': note_path,
        'analysis': analysis,
        'selected_figures': selected_figures,
        'token_usage': token_info_summary
    }


# ========== è§£è€¦çš„OCRå’ŒLLMåˆ†æå‡½æ•° ==========

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨OCRä¸­é—´ç»“æœ
_ocr_cache = {}

def extract_ocr_only(pdf_path: str, paper_info: Dict, category_dir: str) -> Dict[str, Any]:
    """
    ä»…æ‰§è¡ŒOCRé˜¶æ®µï¼Œä¿å­˜OCRç»“æœä¾›åç»­LLMåˆ†æä½¿ç”¨

    Returns:
        åŒ…å«OCRç»“æœçš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
    """
    global token_usage
    token_usage = {
        'ocr_calls': 0,
        'ocr_tokens': 0,
        'llm_calls': 0,
        'llm_tokens_input': 0,
        'llm_tokens_output': 0
    }

    # è¯»å–é…ç½®
    pdf_dpi = analysis_config.get('pdf_dpi', 200)
    max_pages = analysis_config.get('max_pages', 15)
    save_viz = analysis_config.get('save_visualization', True)
    save_cropped = analysis_config.get('save_cropped_figures', True)
    ocr_workers = concurrency_config.get('ocr_workers', 3)

    # åˆ›å»ºè®ºæ–‡ä¸“å±ç›®å½•
    paper_name = os.path.splitext(os.path.basename(pdf_path))[0]
    paper_dir = os.path.join(category_dir, paper_name)

    ocr_dir = os.path.join(paper_dir, "ocr")
    figures_dir = os.path.join(paper_dir, "figures")

    os.makedirs(ocr_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)

    print(f"   ğŸ“„ OCRé˜¶æ®µ: {paper_name}")

    # 1. PDFè½¬å›¾ç‰‡ï¼ˆä¸´æ—¶ç›®å½•ï¼‰
    import tempfile
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"      ğŸ“‘ PDFè½¬å›¾ç‰‡ (DPI={pdf_dpi})...")
        image_paths = pdf_to_images(pdf_path, temp_dir, dpi=pdf_dpi)
        if not image_paths:
            print(f"      âŒ PDFè½¬æ¢å¤±è´¥")
            return None
        print(f"      âœ… PDFè½¬æ¢å®Œæˆ: {len(image_paths)} é¡µ")

        if max_pages is not None and len(image_paths) > max_pages:
            image_paths = image_paths[:max_pages]
            print(f"      â„¹ï¸  é™åˆ¶å¤„ç†å‰ {max_pages} é¡µ")

        # 2. å¹¶å‘OCRåˆ†æ
        ocr_results = []
        all_key_figures = []

        print(f"      ğŸ” OCRè¯†åˆ«ä¸­ (å¹¶å‘={ocr_workers})...")
        tasks = [(i, img_path, ocr_dir, figures_dir, save_viz, save_cropped)
                 for i, img_path in enumerate(image_paths)]

        with ThreadPoolExecutor(max_workers=ocr_workers) as executor:
            futures = {executor.submit(process_single_page, task): task for task in tasks}
            for future in as_completed(futures):
                result = future.result()
                if result and isinstance(result, dict):
                    ocr_results.append(result)
                    all_key_figures.extend(result.get('figures', []))

        ocr_results.sort(key=lambda x: x['page'])
        print(f"      âœ… OCRå®Œæˆ: {len(ocr_results)}/{len(image_paths)} é¡µæˆåŠŸ")

    # ä¿å­˜OCRä¸­é—´ç»“æœ
    ocr_data = {
        'paper_name': paper_name,
        'paper_dir': paper_dir,
        'ocr_dir': ocr_dir,
        'figures_dir': figures_dir,
        'ocr_results': ocr_results,
        'all_key_figures': all_key_figures,
        'token_usage': {
            'ocr_calls': token_usage['ocr_calls'],
            'ocr_tokens': token_usage['ocr_tokens'],
        }
    }

    # ä¿å­˜åˆ°ç¼“å­˜å’Œæ–‡ä»¶
    _ocr_cache[pdf_path] = ocr_data
    ocr_cache_path = os.path.join(paper_dir, "ocr_cache.json")
    with open(ocr_cache_path, 'w', encoding='utf-8') as f:
        json.dump({
            'ocr_results': ocr_results,
            'all_key_figures_count': len(all_key_figures),
            'token_usage': ocr_data['token_usage']
        }, f, ensure_ascii=False, indent=2)

    return ocr_data


def analyze_with_llm(ocr_data: Dict[str, Any], paper_info: Dict, category_dir: str) -> Dict[str, Any]:
    """
    åŸºäºå·²æœ‰çš„OCRç»“æœæ‰§è¡ŒLLMåˆ†æ

    Args:
        ocr_data: extract_ocr_onlyè¿”å›çš„ç»“æœ
        paper_info: è®ºæ–‡ä¿¡æ¯
        category_dir: åˆ†ç±»ç›®å½•

    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
    """
    global token_usage

    # æ¢å¤token_usage
    token_usage = {
        'ocr_calls': ocr_data.get('token_usage', {}).get('ocr_calls', 0),
        'ocr_tokens': ocr_data.get('token_usage', {}).get('ocr_tokens', 0),
        'llm_calls': 0,
        'llm_tokens_input': 0,
        'llm_tokens_output': 0
    }
    start_total = time.time()

    paper_name = ocr_data['paper_name']
    paper_dir = ocr_data['paper_dir']
    ocr_results = ocr_data['ocr_results']
    all_key_figures = ocr_data['all_key_figures']

    print(f"   ğŸ§  LLMé˜¶æ®µ: {paper_name}")

    if not ocr_results:
        print(f"      âŒ æ²¡æœ‰OCRç»“æœå¯ä¾›åˆ†æ")
        return None

    # 3. LLMåˆ†æå†…å®¹
    print(f"      ğŸ’­ æ­£åœ¨è°ƒç”¨LLMåˆ†æ...")
    analysis, llm_token_info = analyze_paper_content(ocr_results)
    print(f"      âœ… LLMåˆ†æå®Œæˆ")

    # 4. é€‰æ‹©å…³é”®å›¾è¡¨
    selected_figures = select_key_figures_for_report(all_key_figures, analysis)
    print(f"      ğŸ“Š é€‰æ‹© {len(selected_figures)} å¼ å…³é”®å›¾è¡¨")

    # 5. ç”Ÿæˆè¯¦ç»†ç¬”è®°
    note_path = os.path.join(paper_dir, "note.md")

    token_info_summary = {
        'ocr_model': ocr_config.get('model', 'deepseek-ocr'),
        'ocr_calls': token_usage['ocr_calls'],
        'ocr_tokens': token_usage['ocr_tokens'],
        'llm_model': config['openai']['model'],
        'llm_calls': token_usage['llm_calls'],
        'llm_tokens_input': token_usage['llm_tokens_input'],
        'llm_tokens_output': token_usage['llm_tokens_output'],
        'total_time': time.time() - start_total
    }

    generate_paper_note(paper_info, analysis, selected_figures, note_path, token_info_summary)
    print(f"      ğŸ“ ç¬”è®°å·²ç”Ÿæˆ")

    # 6. ä¿å­˜åˆ†ææ•°æ®
    analysis_data_path = os.path.join(paper_dir, "analysis.json")
    with open(analysis_data_path, 'w', encoding='utf-8') as f:
        json.dump({
            'paper_info': paper_info,
            'analysis': analysis,
            'selected_figures': selected_figures,
            'all_figures_count': len(all_key_figures),
            'token_usage': token_info_summary
        }, f, ensure_ascii=False, indent=2)

    print(f"      âœ… LLMé˜¶æ®µå®Œæˆ (è€—æ—¶ {token_info_summary['total_time']:.1f}ç§’)")

    return {
        'paper_dir': paper_dir,
        'note_path': note_path,
        'analysis': analysis,
        'selected_figures': selected_figures,
        'token_usage': token_info_summary
    }

"""
è®ºæ–‡æ·±åº¦åˆ†ææ¨¡å— - å¯¹æ„Ÿå…´è¶£çš„è®ºæ–‡è¿›è¡ŒPDFè½¬å›¾ç‰‡ã€OCRåˆ†æã€å›¾æ–‡æŠ¥å‘Šç”Ÿæˆ
æ”¯æŒå¹¶å‘OCRå¤„ç†
"""
import os
import re
import base64
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
from PIL import Image, ImageDraw, ImageFont
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import yaml

with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# è·å–é…ç½®
analysis_config = config.get('analysis', {})
concurrency_config = config.get('concurrency', {})
ocr_config = config.get('deepseek_ocr', {})
openai_config = config.get('openai', {})

# OpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºå†…å®¹åˆ†æï¼‰
client = OpenAI(
    api_key=config['openai']['api_key'],
    base_url=config['openai']['base_url']
)

# DeepSeek OCRå®¢æˆ·ç«¯
if ocr_config.get('api_key') and ocr_config.get('base_url'):
    ocr_client = OpenAI(
        api_key=ocr_config['api_key'],
        base_url=ocr_config['base_url']
    )
else:
    ocr_client = client

# Tokenæ¶ˆè€—è®°å½•
token_usage = {
    'ocr_calls': 0,
    'ocr_tokens': 0,
    'llm_calls': 0,
    'llm_tokens_input': 0,
    'llm_tokens_output': 0
}


def pdf_to_images(pdf_path: str, output_dir: str, dpi: int = None) -> List[str]:
    """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡ï¼Œå®æ—¶æ‰“å°è¿›åº¦"""
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… PyMuPDF: pip install PyMuPDF")
        return []
    
    dpi = dpi or analysis_config.get('pdf_dpi', 200)
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"  ğŸ“„ PDFè½¬å›¾ç‰‡: {os.path.basename(pdf_path)}")
    
    # ç¦ç”¨MuPDFçš„è­¦å‘Šè¾“å‡º
    fitz.set_messages_enabled(False)
    
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    image_paths = []
    
    print(f"     å…± {total_pages} é¡µï¼Œå¼€å§‹è½¬æ¢...")
    
    for page_num in range(total_pages):
        page = doc[page_num]
        mat = fitz.Matrix(dpi/72, dpi/72)
        try:
            pix = page.get_pixmap(matrix=mat)
            image_path = os.path.join(output_dir, f"page_{page_num+1:03d}.png")
            pix.save(image_path)
            image_paths.append(image_path)
            print(f"     âœ“ ç¬¬ {page_num+1}/{total_pages} é¡µè½¬æ¢å®Œæˆ")
        except Exception as e:
            print(f"     âš ï¸ ç¬¬ {page_num+1}/{total_pages} é¡µè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡")
            continue
    
    doc.close()
    print(f"     æˆåŠŸè½¬æ¢ {len(image_paths)}/{total_pages} é¡µ")
    return image_paths


def call_deepseek_ocr(image_path: str) -> Tuple[str, Dict]:
    """è°ƒç”¨DeepSeek-OCRæ¨¡å‹åˆ†æå›¾ç‰‡"""
    global token_usage
    
    timeout = ocr_config.get('timeout', 120)
    max_retries = ocr_config.get('max_retries', 3)
    retry_delay = ocr_config.get('retry_delay', 5)
    
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_image}"},
                    "detail": "high"
                },
                {
                    "type": "text",
                    "text": "<|grounding|>Convert the document to markdown."
                }
            ]
        }
    ]
    
    for attempt in range(max_retries):
        try:
            ocr_model = ocr_config.get('model', 'deepseek-ocr')
            start_time = time.time()
            response = ocr_client.chat.completions.create(
                model=ocr_model,
                messages=messages,
                temperature=0.1,
                timeout=timeout
            )
            elapsed = time.time() - start_time
            
            token_usage['ocr_calls'] += 1
            usage = response.usage
            if usage:
                token_usage['ocr_tokens'] += usage.total_tokens
            
            return response.choices[0].message.content, {
                'model': ocr_model,
                'elapsed': elapsed,
                'tokens': usage.total_tokens if usage else 0
            }
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"       OCRé‡è¯• {attempt+1}/{max_retries}...")
                time.sleep(retry_delay)
            else:
                print(f"       OCRæœ€ç»ˆå¤±è´¥: {e}")
                return None, {}


def process_single_page(args):
    """å¤„ç†å•é¡µï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    page_idx, image_path, ocr_dir, figures_dir, save_viz, save_cropped = args
    page_num = page_idx + 1
    
    # OCRè¯†åˆ«
    ocr_text, ocr_token_info = call_deepseek_ocr(image_path)
    if not ocr_text:
        return None
    
    ocr_items = parse_ocr_response(ocr_text)
    
    # ä¿å­˜OCRæ–‡æœ¬
    ocr_txt_path = os.path.join(ocr_dir, f"page_{page_num:03d}.txt")
    with open(ocr_txt_path, 'w', encoding='utf-8') as f:
        f.write(ocr_text)
    
    # å¯è§†åŒ–
    if save_viz:
        vis_path = os.path.join(ocr_dir, f"page_{page_num:03d}_vis.png")
        visualize_ocr_result(image_path, ocr_items, vis_path)
    
    # æå–å…³é”®å›¾è¡¨
    page_figures = []
    if save_cropped:
        page_figures = extract_key_figures(ocr_items, image_path, figures_dir, page_num)
    
    print(f"     âœ“ ç¬¬ {page_num} é¡µOCRå®Œæˆ")
    
    return {
        'page': page_num,
        'items': ocr_items,
        'raw_text': ocr_text,
        'figures': page_figures
    }


def parse_ocr_response(content: str) -> List[Dict[str, Any]]:
    """è§£æOCRå“åº”ï¼Œæå–å„ä¸ªåŒºåŸŸ"""
    items = []
    tag_pattern = re.compile(r'(?P<type>\w+)\[\[(?P<rect>[\d,\s,]+)\]\]')
    matches = list(tag_pattern.finditer(content))
    
    for i, match in enumerate(matches):
        data = match.groupdict()
        label = data['type']
        rect_str = data['rect']
        
        try:
            bbox = [int(x) for x in re.split(r'[,\s]+', rect_str.strip()) if x]
        except ValueError:
            continue
            
        start_idx = match.end()
        if i < len(matches) - 1:
            end_idx = matches[i+1].start()
        else:
            end_idx = len(content)
            
        text_content = content[start_idx:end_idx].strip()
        
        items.append({
            "type": label,
            "bbox": bbox,
            "text": text_content
        })
    
    return items


def crop_region(image_path: str, bbox: List[int], output_path: str):
    """ä»å›¾ç‰‡ä¸­è£å‰ªæŒ‡å®šåŒºåŸŸ"""
    img = Image.open(image_path)
    width, height = img.size
    
    x1 = int(bbox[0] / 1000 * width)
    y1 = int(bbox[1] / 1000 * height)
    x2 = int(bbox[2] / 1000 * width)
    y2 = int(bbox[3] / 1000 * height)
    
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(width, x2), min(height, y2)
    
    if x2 > x1 and y2 > y1:
        cropped = img.crop((x1, y1, x2, y2))
        cropped.save(output_path)


def visualize_ocr_result(image_path: str, ocr_items: List[Dict], output_path: str):
    """ç»˜åˆ¶OCRå¯è§†åŒ–ç»“æœ - åªåœ¨åŸå›¾ä¸Šç”»æ¡†å’Œæ ‡ç­¾"""
    img = Image.open(image_path).convert("RGB")
    width, height = img.size
    draw = ImageDraw.Draw(img)
    
    try:
        font_size = max(12, int(width / 80))
        font = ImageFont.truetype("/System/Library/Fonts/Supplemental/Arial.ttf", size=font_size)
    except IOError:
        font = ImageFont.load_default()
    
    color_map = {
        "title": (255, 0, 0),
        "text": (0, 0, 0),
        "header": (0, 128, 0),
        "figure": (0, 0, 255),
        "image": (0, 0, 255),
        "image_caption": (255, 165, 0),
        "caption": (255, 165, 0),
        "table": (128, 0, 128),
        "table_caption": (255, 105, 180),
        "sub_title": (0, 128, 128),
        "author": (128, 128, 0),
        "abstract": (70, 130, 180),
        "reference": (105, 105, 105),
        "formula": (255, 20, 147),
        "code": (0, 100, 0),
    }
    
    for item in ocr_items:
        bbox = item['bbox']
        label = item['type']
        
        if len(bbox) == 4:
            x1 = int(bbox[0] / 1000 * width)
            y1 = int(bbox[1] / 1000 * height)
            x2 = int(bbox[2] / 1000 * width)
            y2 = int(bbox[3] / 1000 * height)
        else:
            continue
        
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(width, x2), min(height, y2)
        
        color = color_map.get(label, (100, 100, 100))
        
        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
        text_w = font.getlength(label)
        draw.rectangle([x1, y1 - font_size - 2, x1 + text_w + 4, y1], fill=color)
        draw.text((x1 + 2, y1 - font_size), label, fill=(255, 255, 255), font=font)
    
    img.save(output_path)


def extract_key_figures(ocr_items: List[Dict], image_path: str, 
                        figures_dir: str, page_num: int) -> List[Dict]:
    """æå–å…³é”®å›¾è¡¨"""
    os.makedirs(figures_dir, exist_ok=True)
    
    key_figures = []
    
    for i, item in enumerate(ocr_items):
        label = item['type']
        
        if label not in ['image', 'figure', 'table']:
            continue
        
        caption = ""
        for j in range(i+1, min(i+3, len(ocr_items))):
            if ocr_items[j]['type'] in ['caption', 'image_caption', 'table_caption']:
                caption = ocr_items[j]['text']
                break
        
        ext = 'fig' if label in ['image', 'figure'] else 'table'
        crop_path = os.path.join(figures_dir, f"{ext}_p{page_num:03d}_{i+1:02d}.png")
        crop_region(image_path, item['bbox'], crop_path)
        
        key_figures.append({
            'type': label,
            'page': page_num,
            'index': i + 1,
            'caption': caption,
            'bbox': item['bbox'],
            'crop_path': crop_path,
            'text': item.get('text', '')
        })
    
    return key_figures


def analyze_paper_content(ocr_results: List[Dict]) -> Tuple[Dict, Dict]:
    """ä½¿ç”¨LLMåˆ†æè®ºæ–‡OCRå†…å®¹"""
    global token_usage
    
    timeout = openai_config.get('timeout', 60)
    max_retries = openai_config.get('max_retries', 3)
    retry_delay = openai_config.get('retry_delay', 5)
    max_length = analysis_config.get('max_ocr_text_length', 12000)
    
    # åˆå¹¶æ–‡æœ¬
    all_text = ""
    for page in ocr_results:
        page_num = page['page']
        all_text += f"\n\n=== Page {page_num} ===\n\n"
        for item in page['items']:
            all_text += f"[{item['type']}] {item['text']}\n"
    
    if len(all_text) > max_length:
        all_text = all_text[:max_length] + "\n... (å†…å®¹å·²æˆªæ–­)"
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹è®ºæ–‡çš„OCRå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

è®ºæ–‡å†…å®¹:
{all_text}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›:
{{
    "title": "è®ºæ–‡æ ‡é¢˜",
    "title_cn": "è®ºæ–‡ä¸­æ–‡æ ‡é¢˜æˆ–ç¿»è¯‘",
    "authors": ["ä½œè€…1", "ä½œè€…2"],
    "abstract": "æ‘˜è¦å†…å®¹",
    "core_problem": "æ ¸å¿ƒé—®é¢˜æè¿°ï¼Œç”¨1-2å¥è¯æ¦‚æ‹¬",
    "core_contribution": "æ ¸å¿ƒè´¡çŒ®ï¼Œåˆ†ç‚¹åˆ—å‡º",
    "method_summary": "æ–¹æ³•æ¦‚è¿°ï¼ŒåŒ…å«å…³é”®æŠ€æœ¯åˆ›æ–°",
    "key_figures_description": ["å›¾1æè¿°: è¿™æ˜¯ä»€ä¹ˆå›¾ï¼Œå±•ç¤ºäº†ä»€ä¹ˆ", "å›¾2æè¿°: ..."],
    "key_results": "ä¸»è¦å®éªŒç»“æœ",
    "key_tables": ["è¡¨1: æè¿°è¡¨æ ¼å†…å®¹å’Œå…³é”®æ•°æ®"],
    "conclusion": "ç»“è®º"
}}

æ³¨æ„ï¼š
1. è¿”å›å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
2. æ‰€æœ‰æè¿°ä½¿ç”¨ä¸­æ–‡
3. å¯¹å›¾è¡¨çš„æè¿°è¦è¯¦ç»†ï¼Œè¯´æ˜å…¶ç”¨é€”å’Œå±•ç¤ºçš„å†…å®¹
"""
    
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            response = client.chat.completions.create(
                model=config['openai']['model'],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=timeout
            )
            elapsed = time.time() - start_time
            
            token_usage['llm_calls'] += 1
            usage = response.usage
            if usage:
                token_usage['llm_tokens_input'] += usage.prompt_tokens
                token_usage['llm_tokens_output'] += usage.completion_tokens
            
            result = json.loads(response.choices[0].message.content)
            token_info = {
                'model': config['openai']['model'],
                'elapsed': elapsed,
                'tokens_input': usage.prompt_tokens if usage else 0,
                'tokens_output': usage.completion_tokens if usage else 0
            }
            return result, token_info
        except Exception as e:
            print(f"  å†…å®¹åˆ†æå°è¯• {attempt+1}/{max_retries} å¤±è´¥: {e}")
            if attempt == max_retries - 1:
                print(f"  å†…å®¹åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                return {}, {}
            time.sleep(retry_delay)


def select_key_figures_for_report(all_figures: List[Dict], analysis: Dict) -> List[Dict]:
    """é€‰æ‹©æœ€å…³é”®çš„å›¾è¡¨ - æ™ºèƒ½åˆ†ç±»é€‰æ‹©"""
    if not all_figures:
        return []
    
    max_figures = analysis_config.get('max_figures_per_paper', 4)
    selected = []
    
    # åˆ†ç±»ï¼šæ¶æ„å›¾ã€ç»“æœå›¾ã€è¡¨æ ¼
    arch_figures = []
    result_figures = []
    tables = []
    
    for fig in all_figures:
        caption = fig.get('caption', '').lower()
        fig_type = fig['type']
        
        # æ¶æ„å›¾å…³é”®è¯
        arch_keywords = ['arch', 'framework', 'overview', 'model', 'structure', 'pipeline', 'system', 'design']
        # ç»“æœå›¾å…³é”®è¯
        result_keywords = ['result', 'performance', 'comparison', 'ablation', 'accuracy', 'loss', 'curve', 'plot']
        
        if fig_type == 'table':
            tables.append(fig)
        elif any(kw in caption for kw in arch_keywords):
            arch_figures.append(fig)
        elif any(kw in caption for kw in result_keywords):
            result_figures.append(fig)
        else:
            # å…¶ä»–å›¾ç‰‡ï¼Œå½’å…¥ç»“æœå›¾
            result_figures.append(fig)
    
    # é€‰æ‹©ï¼š1-2å¼ æ¶æ„å›¾ï¼Œ1-2å¼ ç»“æœå›¾ï¼Œ1å¼ è¡¨æ ¼
    selected.extend(arch_figures[:2])
    selected.extend(result_figures[:2])
    selected.extend(tables[:1])
    
    # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……å…¶ä»–å›¾
    remaining = [f for f in all_figures if f not in selected]
    selected.extend(remaining[:max_figures - len(selected)])
    
    # æ·»åŠ LLMåˆ†ææè¿°
    figure_descriptions = analysis.get('key_figures_description', [])
    for i, fig in enumerate(selected):
        if i < len(figure_descriptions):
            fig['analysis_desc'] = figure_descriptions[i]
    
    return selected[:max_figures]


def generate_paper_note(paper_info: Dict, analysis: Dict, selected_figures: List[Dict],
                        output_path: str, token_info: Dict):
    """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„è¯¦ç»†ç¬”è®° - å›¾æ–‡å¹¶èŒ‚ï¼Œå›¾è¡¨èå…¥å†…å®¹"""
    title = analysis.get('title', paper_info['title'])
    title_cn = analysis.get('title_cn', '')
    
    md_content = f"# {title}\n\n"
    
    if title_cn:
        md_content += f"**ä¸­æ–‡æ ‡é¢˜**: {title_cn}\n\n"
    
    md_content += f"**ä½œè€…**: {', '.join(analysis.get('authors', paper_info['authors']))}\n\n"
    md_content += f"**æ¥æº**: arXiv | **æ—¥æœŸ**: {paper_info.get('date', '')}\n\n"
    md_content += "---\n\n"
    
    md_content += "## æ ¸å¿ƒé—®é¢˜\n\n"
    md_content += f"{analysis.get('core_problem', 'æœªæå–')}\n\n"
    
    md_content += "## æ ¸å¿ƒè´¡çŒ®\n\n"
    contribution = analysis.get('core_contribution', '')
    if isinstance(contribution, list):
        for item in contribution:
            md_content += f"- {item}\n"
    else:
        md_content += f"{contribution}\n"
    md_content += "\n"
    
    md_content += "## æ–¹æ³•æ¦‚è¿°\n\n"
    md_content += f"{analysis.get('method_summary', 'æœªæå–')}\n\n"
    
    # èå…¥æ¶æ„å›¾
    arch_figures = [f for f in selected_figures if f['type'] in ['image', 'figure'] and 
                   any(kw in f.get('caption', '').lower() for kw in ['arch', 'framework', 'overview', 'model', 'structure'])]
    if arch_figures:
        md_content += "### æ¶æ„å›¾\n\n"
        for fig in arch_figures[:2]:
            rel_path = os.path.basename(fig['crop_path'])
            desc = fig.get('analysis_desc', fig.get('caption', ''))
            if desc:
                md_content += f"{desc}\n\n"
            md_content += f"![æ¶æ„å›¾]({rel_path})\n\n"
    
    md_content += "## å®éªŒç»“æœ\n\n"
    md_content += f"{analysis.get('key_results', 'æœªæå–')}\n\n"
    
    # èå…¥ç»“æœå›¾å’Œè¡¨æ ¼
    result_figures = [f for f in selected_figures if 
                     (f['type'] in ['image', 'figure'] and any(kw in f.get('caption', '').lower() for kw in ['result', 'performance', 'comparison', 'ablation'])) or
                     f['type'] == 'table']
    
    if result_figures:
        md_content += "### å®éªŒæ•°æ®\n\n"
        for fig in result_figures[:3]:
            rel_path = os.path.basename(fig['crop_path'])
            caption = fig.get('caption', '')
            desc = fig.get('analysis_desc', '')
            
            if fig['type'] == 'table':
                md_content += f"**{caption or 'æ•°æ®è¡¨'}**\n\n"
            else:
                md_content += f"**{caption or 'ç»“æœå›¾'}**\n\n"
            
            if desc:
                md_content += f"{desc}\n\n"
            
            md_content += f"![{caption}]({rel_path})\n\n"
    
    md_content += "## ç»“è®º\n\n"
    md_content += f"{analysis.get('conclusion', 'æœªæå–')}\n\n"
    
    md_content += "---\n\n"
    md_content += "## ä¸ªäººæ€è€ƒ\n\n"
    md_content += "### äº®ç‚¹\n\n- \n\n"
    md_content += "### å±€é™æ€§\n\n- \n\n"
    md_content += "### å¯å‘\n\n- \n\n"
    
    md_content += "---\n\n"
    md_content += "## å¤„ç†è®°å½•\n\n"
    md_content += f"- OCRæ¨¡å‹: {token_info.get('ocr_model', 'unknown')}\n"
    md_content += f"- OCRè°ƒç”¨æ¬¡æ•°: {token_info.get('ocr_calls', 0)}\n"
    md_content += f"- OCRæ€»tokens: {token_info.get('ocr_tokens', 0)}\n"
    md_content += f"- LLMæ¨¡å‹: {token_info.get('llm_model', 'unknown')}\n"
    md_content += f"- LLMè°ƒç”¨æ¬¡æ•°: {token_info.get('llm_calls', 0)}\n"
    md_content += f"- LLMè¾“å…¥tokens: {token_info.get('llm_tokens_input', 0)}\n"
    md_content += f"- LLMè¾“å‡ºtokens: {token_info.get('llm_tokens_output', 0)}\n"
    md_content += f"- å¤„ç†æ—¶é—´: {token_info.get('total_time', 0):.2f}ç§’\n"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    return md_content


def analyze_paper_deep(pdf_path: str, paper_info: Dict, category_dir: str) -> Dict[str, Any]:
    """
    å¯¹è®ºæ–‡è¿›è¡Œæ·±åº¦åˆ†æçš„ä¸»å‡½æ•° - æ”¯æŒå¹¶å‘OCR
    
    ç›®å½•ç»“æ„:
    Category/
    â””â”€â”€ Author_Title/
        â”œâ”€â”€ paper.pdf
        â”œâ”€â”€ note.md
        â”œâ”€â”€ ocr/
        â”‚   â”œâ”€â”€ page_001.txt
        â”‚   â””â”€â”€ page_001_vis.png
        â”œâ”€â”€ figures/
        â”‚   â”œâ”€â”€ fig_p001_01.png
        â”‚   â””â”€â”€ table_p001_02.png
        â””â”€â”€ analysis.json
    """
    global token_usage
    token_usage = {
        'ocr_calls': 0,
        'ocr_tokens': 0,
        'llm_calls': 0,
        'llm_tokens_input': 0,
        'llm_tokens_output': 0
    }
    start_total = time.time()
    
    # è¯»å–é…ç½®
    pdf_dpi = analysis_config.get('pdf_dpi', 200)
    max_pages = analysis_config.get('max_pages', 15)
    save_viz = analysis_config.get('save_visualization', True)
    save_cropped = analysis_config.get('save_cropped_figures', True)
    ocr_workers = concurrency_config.get('ocr_workers', 3)
    
    # åˆ›å»ºè®ºæ–‡ä¸“å±ç›®å½•
    paper_name = os.path.splitext(os.path.basename(pdf_path))[0]
    paper_dir = os.path.join(category_dir, paper_name)
    
    ocr_dir = os.path.join(paper_dir, "ocr")
    figures_dir = os.path.join(paper_dir, "figures")
    
    os.makedirs(ocr_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)
    
    print(f"\n  ğŸ“ è®ºæ–‡ç›®å½•: {paper_dir}")
    
    # 1. PDFè½¬å›¾ç‰‡ï¼ˆä¸´æ—¶ç›®å½•ï¼‰
    import tempfile
    with tempfile.TemporaryDirectory() as temp_dir:
        image_paths = pdf_to_images(pdf_path, temp_dir, dpi=pdf_dpi)
        if not image_paths:
            print("  âŒ PDFè½¬æ¢å¤±è´¥")
            return None
        
        if len(image_paths) > max_pages:
            print(f"  âš ï¸ è®ºæ–‡å…± {len(image_paths)} é¡µï¼Œåªå¤„ç†å‰ {max_pages} é¡µ")
            image_paths = image_paths[:max_pages]
        
        # 2. å¹¶å‘OCRåˆ†æ
        print(f"  ğŸ” OCRåˆ†æï¼ˆå¹¶å‘{ocr_workers}é¡µï¼‰...")
        ocr_results = []
        all_key_figures = []
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = [(i, img_path, ocr_dir, figures_dir, save_viz, save_cropped) 
                 for i, img_path in enumerate(image_paths)]
        
        # å¹¶å‘æ‰§è¡ŒOCR
        with ThreadPoolExecutor(max_workers=ocr_workers) as executor:
            futures = {executor.submit(process_single_page, task): task for task in tasks}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    ocr_results.append(result)
                    all_key_figures.extend(result.get('figures', []))
        
        # æŒ‰é¡µç æ’åº
        ocr_results.sort(key=lambda x: x['page'])
    
    # 3. LLMåˆ†æå†…å®¹
    print("  ğŸ§  å†…å®¹åˆ†æ...")
    analysis, llm_token_info = analyze_paper_content(ocr_results)
    
    # 4. é€‰æ‹©å…³é”®å›¾è¡¨
    selected_figures = select_key_figures_for_report(all_key_figures, analysis)
    
    # 5. ç”Ÿæˆè¯¦ç»†ç¬”è®°
    print("  ğŸ“ ç”Ÿæˆç¬”è®°...")
    note_path = os.path.join(paper_dir, "note.md")
    
    token_info_summary = {
        'ocr_model': ocr_config.get('model', 'deepseek-ocr'),
        'ocr_calls': token_usage['ocr_calls'],
        'ocr_tokens': token_usage['ocr_tokens'],
        'llm_model': config['openai']['model'],
        'llm_calls': token_usage['llm_calls'],
        'llm_tokens_input': token_usage['llm_tokens_input'],
        'llm_tokens_output': token_usage['llm_tokens_output'],
        'total_time': time.time() - start_total
    }
    
    generate_paper_note(paper_info, analysis, selected_figures, note_path, token_info_summary)
    
    # 6. ä¿å­˜åˆ†ææ•°æ®
    analysis_data_path = os.path.join(paper_dir, "analysis.json")
    with open(analysis_data_path, 'w', encoding='utf-8') as f:
        json.dump({
            'paper_info': paper_info,
            'analysis': analysis,
            'selected_figures': selected_figures,
            'all_figures_count': len(all_key_figures),
            'token_usage': token_info_summary
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… å®Œæˆ! å›¾è¡¨: {len(selected_figures)}ä¸ª, è€—æ—¶: {token_info_summary['total_time']:.1f}ç§’")
    
    return {
        'paper_dir': paper_dir,
        'note_path': note_path,
        'analysis': analysis,
        'selected_figures': selected_figures,
        'token_usage': token_info_summary
    }
